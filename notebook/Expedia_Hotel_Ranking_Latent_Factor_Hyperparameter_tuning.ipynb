{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expedia Hotel Ranking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to construct models to rank properties based on customers' preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Import Packages and Load the Data](#Import-Packages-and-Load-the-Data)  \n",
    "2. [Data Exploration](#Data-Exploration) \n",
    "    1. [Goals](#Goals)  \n",
    "    2. [Data Size and Missing Data](#Data-Size-and-Missing-Data) \n",
    "    3. [Summary Statistics](#Summary-statistics) \n",
    "    4. [Investigate property room capacity](#Investigate-negative-property-room-capcity)  \n",
    "    5. [Check property price column](#Use-correlations-to-check-if-the-property-price-is-per-night-per-room) \n",
    "    6. [Check frequency of search query](#Check-frequency-of-number-of-search-queries-in-the-dataset)   \n",
    "3. [Model and Process the Data - Latent Factor Model](#Model-Selection-and-Process-the-Data:-Latent-Factor-Model) \n",
    "    1. [Feature Engineering](#Feature-engineering)\n",
    "    2. [User-item interaction matrix](#Create-user-item-matrix)\n",
    "    3. [Define user parameters](#Define-user-and-property-parameters)\n",
    "    4. [Implicit Feedback with logistic probabilistic model](#Implicit-feedback-recommendation-using-logistic-latent-Factor-model) \n",
    "4. [Latent Factor, Implicit Feedback, Logistic Probabilistic Model](#Implicit-feedback-recommendation-using-logistic-latent-Factor-model)\n",
    "    1. [Model Training](#Initialize-the-model,-train-the-model,-and-get-user-and-item-latent-vectors)\n",
    "    2. [Model Prediction](#Model-Prediction)\n",
    "    3. [Model Evaluation](#Model-Evaluation)\n",
    "    4. [Streamline functions for model training, prediction, and evaluation](#Put-model-training-and-evaluation-functions-together) \n",
    "5. [Ensemble Model: incorporating popularity, price, and reviews](#Ensemble-Model)\n",
    "    1. [Hyperparameter Tuning](#Weights-tuning-based-on-grid-search)\n",
    "    2. [Use tuned parameters to produce the final result](#Generate-results-for-the-entire-training-set-based-on-selected-parameters) \n",
    "6. [Challenges and Next Steps](#Challanges-and-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages and Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm, notebook\n",
    "from joblib import Parallel, delayed \n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.join(ROOT_PATH, \"data\")\n",
    "OUTPUT_PATH = os.path.join(ROOT_PATH, \"output\")\n",
    "train_data_file = os.path.join(DATA_PATH, \"train.csv\")\n",
    "test_data_file = os.path.join(DATA_PATH, \"test.csv\")\n",
    "\n",
    "train = pd.read_csv(train_data_file)\n",
    "test = pd.read_csv(test_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "### Goals\n",
    "- What is the shape of the data?  <br>\n",
    "- Is there any missing data?  <br>\n",
    "- Is there any abnormal data that needs to be fixed or removed? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Size and Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the training set: (841115, 47)\n",
      "shape of the test set: (351544, 46)\n",
      "unique number of search ids: 24013\n",
      "unique number of visitors: 23101\n",
      "unique number of properties: 6330\n",
      "overlapping visitors: 614\n",
      "overlapping search ids: 0\n"
     ]
    }
   ],
   "source": [
    "# check number of datapoints\n",
    "print(\"shape of the training set: \" + str(train.shape))\n",
    "print(\"shape of the test set: \" + str(test.shape))\n",
    "\n",
    "print(\"unique number of search ids: \" + str(train.srch_id.nunique()))\n",
    "print(\"unique number of visitors: \" + str(train.srch_visitor_id.nunique()))\n",
    "print(\"unique number of properties: \" + str(train.prop_key.nunique()))\n",
    "\n",
    "# check overlap between train and test set\n",
    "print(\"overlapping visitors: \" + str(len(set(train.srch_visitor_id.unique()).intersection(test.srch_visitor_id.unique()))))\n",
    "print(\"overlapping search ids: \" + str(len(set(train.srch_id.unique()).intersection(test.srch_id.unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>prop_key</th>\n",
       "      <th>srch_date_time</th>\n",
       "      <th>srch_visitor_id</th>\n",
       "      <th>srch_visitor_visit_nbr</th>\n",
       "      <th>srch_visitor_loc_country</th>\n",
       "      <th>srch_visitor_loc_region</th>\n",
       "      <th>srch_visitor_loc_city</th>\n",
       "      <th>srch_visitor_wr_member</th>\n",
       "      <th>srch_posa_continent</th>\n",
       "      <th>srch_posa_country</th>\n",
       "      <th>srch_hcom_destination_id</th>\n",
       "      <th>srch_dest_longitude</th>\n",
       "      <th>srch_dest_latitude</th>\n",
       "      <th>srch_ci</th>\n",
       "      <th>srch_co</th>\n",
       "      <th>srch_ci_day</th>\n",
       "      <th>srch_co_day</th>\n",
       "      <th>srch_los</th>\n",
       "      <th>srch_bw</th>\n",
       "      <th>srch_adults_cnt</th>\n",
       "      <th>srch_children_cnt</th>\n",
       "      <th>srch_rm_cnt</th>\n",
       "      <th>srch_mobile_bool</th>\n",
       "      <th>srch_mobile_app</th>\n",
       "      <th>srch_device</th>\n",
       "      <th>srch_currency</th>\n",
       "      <th>prop_travelad_bool</th>\n",
       "      <th>prop_dotd_bool</th>\n",
       "      <th>prop_price_without_discount_local</th>\n",
       "      <th>prop_price_without_discount_usd</th>\n",
       "      <th>prop_price_with_discount_local</th>\n",
       "      <th>prop_price_with_discount_usd</th>\n",
       "      <th>prop_imp_drr</th>\n",
       "      <th>prop_booking_bool</th>\n",
       "      <th>prop_brand_bool</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_super_region</th>\n",
       "      <th>prop_continent</th>\n",
       "      <th>prop_country</th>\n",
       "      <th>prop_market_id</th>\n",
       "      <th>prop_submarket_id</th>\n",
       "      <th>prop_room_capacity</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>prop_review_count</th>\n",
       "      <th>prop_hostel_bool</th>\n",
       "      <th>srch_local_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1046322713</td>\n",
       "      <td>257690</td>\n",
       "      <td>2014-09-13 18:37:32</td>\n",
       "      <td>9373b009-4e10-495a-afae-204dd1fe4b7c</td>\n",
       "      <td>5</td>\n",
       "      <td>TWN</td>\n",
       "      <td>TPE</td>\n",
       "      <td>TAIPEI</td>\n",
       "      <td>Signed in - Persistent|WR Member|Remembered FC...</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>TAIWAN, REPUBLIC OF CHINA</td>\n",
       "      <td>728660</td>\n",
       "      <td>135.498154</td>\n",
       "      <td>34.701897</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>164</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DESKTOP</td>\n",
       "      <td>TWD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5605.0</td>\n",
       "      <td>186.72</td>\n",
       "      <td>3356.0</td>\n",
       "      <td>111.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>APAC</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>JAPAN</td>\n",
       "      <td>60041</td>\n",
       "      <td>109140</td>\n",
       "      <td>575</td>\n",
       "      <td>4.1</td>\n",
       "      <td>403.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1046322713</td>\n",
       "      <td>3066218</td>\n",
       "      <td>2014-09-13 18:37:32</td>\n",
       "      <td>9373b009-4e10-495a-afae-204dd1fe4b7c</td>\n",
       "      <td>5</td>\n",
       "      <td>TWN</td>\n",
       "      <td>TPE</td>\n",
       "      <td>TAIPEI</td>\n",
       "      <td>Signed in - Persistent|WR Member|Remembered FC...</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>TAIWAN, REPUBLIC OF CHINA</td>\n",
       "      <td>728660</td>\n",
       "      <td>135.498154</td>\n",
       "      <td>34.701897</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>164</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DESKTOP</td>\n",
       "      <td>TWD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4614.0</td>\n",
       "      <td>153.71</td>\n",
       "      <td>2769.0</td>\n",
       "      <td>92.24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>APAC</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>JAPAN</td>\n",
       "      <td>60041</td>\n",
       "      <td>109140</td>\n",
       "      <td>339</td>\n",
       "      <td>3.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1046322713</td>\n",
       "      <td>2271987</td>\n",
       "      <td>2014-09-13 18:37:32</td>\n",
       "      <td>9373b009-4e10-495a-afae-204dd1fe4b7c</td>\n",
       "      <td>5</td>\n",
       "      <td>TWN</td>\n",
       "      <td>TPE</td>\n",
       "      <td>TAIPEI</td>\n",
       "      <td>Signed in - Persistent|WR Member|Remembered FC...</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>TAIWAN, REPUBLIC OF CHINA</td>\n",
       "      <td>728660</td>\n",
       "      <td>135.498154</td>\n",
       "      <td>34.701897</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>164</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DESKTOP</td>\n",
       "      <td>TWD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14026.0</td>\n",
       "      <td>467.25</td>\n",
       "      <td>2821.0</td>\n",
       "      <td>93.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>APAC</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>JAPAN</td>\n",
       "      <td>60041</td>\n",
       "      <td>109140</td>\n",
       "      <td>179</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1046322713</td>\n",
       "      <td>3308025</td>\n",
       "      <td>2014-09-13 18:37:32</td>\n",
       "      <td>9373b009-4e10-495a-afae-204dd1fe4b7c</td>\n",
       "      <td>5</td>\n",
       "      <td>TWN</td>\n",
       "      <td>TPE</td>\n",
       "      <td>TAIPEI</td>\n",
       "      <td>Signed in - Persistent|WR Member|Remembered FC...</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>TAIWAN, REPUBLIC OF CHINA</td>\n",
       "      <td>728660</td>\n",
       "      <td>135.498154</td>\n",
       "      <td>34.701897</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>164</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DESKTOP</td>\n",
       "      <td>TWD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14308.0</td>\n",
       "      <td>476.65</td>\n",
       "      <td>5202.0</td>\n",
       "      <td>173.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>APAC</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>JAPAN</td>\n",
       "      <td>60041</td>\n",
       "      <td>98278</td>\n",
       "      <td>272</td>\n",
       "      <td>4.8</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1046322713</td>\n",
       "      <td>3222046</td>\n",
       "      <td>2014-09-13 18:37:32</td>\n",
       "      <td>9373b009-4e10-495a-afae-204dd1fe4b7c</td>\n",
       "      <td>5</td>\n",
       "      <td>TWN</td>\n",
       "      <td>TPE</td>\n",
       "      <td>TAIPEI</td>\n",
       "      <td>Signed in - Persistent|WR Member|Remembered FC...</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>TAIWAN, REPUBLIC OF CHINA</td>\n",
       "      <td>728660</td>\n",
       "      <td>135.498154</td>\n",
       "      <td>34.701897</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>164</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DESKTOP</td>\n",
       "      <td>TWD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5445.0</td>\n",
       "      <td>181.39</td>\n",
       "      <td>2589.0</td>\n",
       "      <td>86.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>APAC</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>JAPAN</td>\n",
       "      <td>60041</td>\n",
       "      <td>109140</td>\n",
       "      <td>198</td>\n",
       "      <td>3.9</td>\n",
       "      <td>702.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      srch_id  prop_key       srch_date_time  \\\n",
       "0 -1046322713    257690  2014-09-13 18:37:32   \n",
       "1 -1046322713   3066218  2014-09-13 18:37:32   \n",
       "2 -1046322713   2271987  2014-09-13 18:37:32   \n",
       "3 -1046322713   3308025  2014-09-13 18:37:32   \n",
       "4 -1046322713   3222046  2014-09-13 18:37:32   \n",
       "\n",
       "                        srch_visitor_id  srch_visitor_visit_nbr  \\\n",
       "0  9373b009-4e10-495a-afae-204dd1fe4b7c                       5   \n",
       "1  9373b009-4e10-495a-afae-204dd1fe4b7c                       5   \n",
       "2  9373b009-4e10-495a-afae-204dd1fe4b7c                       5   \n",
       "3  9373b009-4e10-495a-afae-204dd1fe4b7c                       5   \n",
       "4  9373b009-4e10-495a-afae-204dd1fe4b7c                       5   \n",
       "\n",
       "  srch_visitor_loc_country srch_visitor_loc_region srch_visitor_loc_city  \\\n",
       "0                      TWN                     TPE                TAIPEI   \n",
       "1                      TWN                     TPE                TAIPEI   \n",
       "2                      TWN                     TPE                TAIPEI   \n",
       "3                      TWN                     TPE                TAIPEI   \n",
       "4                      TWN                     TPE                TAIPEI   \n",
       "\n",
       "                              srch_visitor_wr_member srch_posa_continent  \\\n",
       "0  Signed in - Persistent|WR Member|Remembered FC...                ASIA   \n",
       "1  Signed in - Persistent|WR Member|Remembered FC...                ASIA   \n",
       "2  Signed in - Persistent|WR Member|Remembered FC...                ASIA   \n",
       "3  Signed in - Persistent|WR Member|Remembered FC...                ASIA   \n",
       "4  Signed in - Persistent|WR Member|Remembered FC...                ASIA   \n",
       "\n",
       "           srch_posa_country  srch_hcom_destination_id  srch_dest_longitude  \\\n",
       "0  TAIWAN, REPUBLIC OF CHINA                    728660           135.498154   \n",
       "1  TAIWAN, REPUBLIC OF CHINA                    728660           135.498154   \n",
       "2  TAIWAN, REPUBLIC OF CHINA                    728660           135.498154   \n",
       "3  TAIWAN, REPUBLIC OF CHINA                    728660           135.498154   \n",
       "4  TAIWAN, REPUBLIC OF CHINA                    728660           135.498154   \n",
       "\n",
       "   srch_dest_latitude     srch_ci     srch_co  srch_ci_day  srch_co_day  \\\n",
       "0           34.701897  2015-02-25  2015-02-28            4            7   \n",
       "1           34.701897  2015-02-25  2015-02-28            4            7   \n",
       "2           34.701897  2015-02-25  2015-02-28            4            7   \n",
       "3           34.701897  2015-02-25  2015-02-28            4            7   \n",
       "4           34.701897  2015-02-25  2015-02-28            4            7   \n",
       "\n",
       "   srch_los  srch_bw  srch_adults_cnt  srch_children_cnt  srch_rm_cnt  \\\n",
       "0         3      164              2.0                0.0            1   \n",
       "1         3      164              2.0                0.0            1   \n",
       "2         3      164              2.0                0.0            1   \n",
       "3         3      164              2.0                0.0            1   \n",
       "4         3      164              2.0                0.0            1   \n",
       "\n",
       "   srch_mobile_bool  srch_mobile_app srch_device srch_currency  \\\n",
       "0                 0                0     DESKTOP           TWD   \n",
       "1                 0                0     DESKTOP           TWD   \n",
       "2                 0                0     DESKTOP           TWD   \n",
       "3                 0                0     DESKTOP           TWD   \n",
       "4                 0                0     DESKTOP           TWD   \n",
       "\n",
       "   prop_travelad_bool  prop_dotd_bool  prop_price_without_discount_local  \\\n",
       "0                   0               0                             5605.0   \n",
       "1                   0               0                             4614.0   \n",
       "2                   0               0                            14026.0   \n",
       "3                   0               0                            14308.0   \n",
       "4                   0               0                             5445.0   \n",
       "\n",
       "   prop_price_without_discount_usd  prop_price_with_discount_local  \\\n",
       "0                           186.72                          3356.0   \n",
       "1                           153.71                          2769.0   \n",
       "2                           467.25                          2821.0   \n",
       "3                           476.65                          5202.0   \n",
       "4                           181.39                          2589.0   \n",
       "\n",
       "   prop_price_with_discount_usd  prop_imp_drr  prop_booking_bool  \\\n",
       "0                        111.80             0                  0   \n",
       "1                         92.24             1                  0   \n",
       "2                         93.98             0                  0   \n",
       "3                        173.30             0                  0   \n",
       "4                         86.25             0                  0   \n",
       "\n",
       "   prop_brand_bool  prop_starrating prop_super_region prop_continent  \\\n",
       "0                1              3.5              APAC           ASIA   \n",
       "1                0              3.0              APAC           ASIA   \n",
       "2                1              3.5              APAC           ASIA   \n",
       "3                1              5.0              APAC           ASIA   \n",
       "4                0              3.0              APAC           ASIA   \n",
       "\n",
       "  prop_country  prop_market_id  prop_submarket_id  prop_room_capacity  \\\n",
       "0        JAPAN           60041             109140                 575   \n",
       "1        JAPAN           60041             109140                 339   \n",
       "2        JAPAN           60041             109140                 179   \n",
       "3        JAPAN           60041              98278                 272   \n",
       "4        JAPAN           60041             109140                 198   \n",
       "\n",
       "   prop_review_score  prop_review_count  prop_hostel_bool srch_local_date  \n",
       "0                4.1              403.0                 0      2014-09-13  \n",
       "1                3.6              101.0                 0      2014-09-13  \n",
       "2                4.1             1189.0                 0      2014-09-13  \n",
       "3                4.8              221.0                 0      2014-09-13  \n",
       "4                3.9              702.0                 0      2014-09-13  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prop_price_with_discount_local': 51,\n",
       " 'prop_price_with_discount_usd': 51,\n",
       " 'prop_price_without_discount_local': 51,\n",
       " 'prop_price_without_discount_usd': 51,\n",
       " 'prop_review_count': 8,\n",
       " 'prop_review_score': 8,\n",
       " 'srch_adults_cnt': 18,\n",
       " 'srch_children_cnt': 18,\n",
       " 'srch_currency': 134104,\n",
       " 'srch_posa_continent': 485248,\n",
       " 'srch_visitor_loc_region': 123,\n",
       " 'srch_visitor_wr_member': 444878}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing data\n",
    "{column: train[column].isnull().sum() for column in train.columns if train[column].isnull().sum() > 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prop_price_with_discount_local': 11,\n",
       " 'prop_price_with_discount_usd': 11,\n",
       " 'prop_price_without_discount_local': 11,\n",
       " 'prop_price_without_discount_usd': 11,\n",
       " 'prop_review_count': 6,\n",
       " 'prop_review_score': 6,\n",
       " 'srch_currency': 53400,\n",
       " 'srch_posa_continent': 202751,\n",
       " 'srch_visitor_loc_region': 111,\n",
       " 'srch_visitor_wr_member': 181968}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{column: test[column].isnull().sum() for column in test.columns if test[column].isnull().sum() > 0 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observstions\n",
    "For columns with missing data: <br>\n",
    "- srch_visitor_wr_member needs to be removed because more than 20% of the data is missing. <br>\n",
    "- srch_visitor_loc_region, srch_posa_continent, srch_currency, prop_price_without_discount_local, prop_price_without_discount_usd, prop_price_with_discount_local columns can be safely removed as their information is covered by other columns. <br>\n",
    "- The missing data in prop_price_with_discount_usd, prop_review_count, prop_review_score, srch_adults_cnt, and srch_children_cnt columns need to be imputed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>prop_key</th>\n",
       "      <th>srch_visitor_visit_nbr</th>\n",
       "      <th>srch_hcom_destination_id</th>\n",
       "      <th>srch_dest_longitude</th>\n",
       "      <th>srch_dest_latitude</th>\n",
       "      <th>srch_ci_day</th>\n",
       "      <th>srch_co_day</th>\n",
       "      <th>srch_los</th>\n",
       "      <th>srch_bw</th>\n",
       "      <th>srch_adults_cnt</th>\n",
       "      <th>srch_children_cnt</th>\n",
       "      <th>srch_rm_cnt</th>\n",
       "      <th>srch_mobile_bool</th>\n",
       "      <th>srch_mobile_app</th>\n",
       "      <th>prop_travelad_bool</th>\n",
       "      <th>prop_dotd_bool</th>\n",
       "      <th>prop_price_without_discount_local</th>\n",
       "      <th>prop_price_without_discount_usd</th>\n",
       "      <th>prop_price_with_discount_local</th>\n",
       "      <th>prop_price_with_discount_usd</th>\n",
       "      <th>prop_imp_drr</th>\n",
       "      <th>prop_booking_bool</th>\n",
       "      <th>prop_brand_bool</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_market_id</th>\n",
       "      <th>prop_submarket_id</th>\n",
       "      <th>prop_room_capacity</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>prop_review_count</th>\n",
       "      <th>prop_hostel_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.411150e+05</td>\n",
       "      <td>8.411150e+05</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>8.411150e+05</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841097.000000</td>\n",
       "      <td>841097.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.0</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>8.410640e+05</td>\n",
       "      <td>8.410640e+05</td>\n",
       "      <td>8.410640e+05</td>\n",
       "      <td>8.410640e+05</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "      <td>841107.000000</td>\n",
       "      <td>841107.000000</td>\n",
       "      <td>841115.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.724233e+07</td>\n",
       "      <td>8.185569e+05</td>\n",
       "      <td>12.325977</td>\n",
       "      <td>1.194309e+06</td>\n",
       "      <td>-45.892803</td>\n",
       "      <td>40.445932</td>\n",
       "      <td>4.272102</td>\n",
       "      <td>3.689650</td>\n",
       "      <td>2.677238</td>\n",
       "      <td>35.349157</td>\n",
       "      <td>2.019900</td>\n",
       "      <td>0.161467</td>\n",
       "      <td>1.113059</td>\n",
       "      <td>0.145173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012221</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>1.204508e+04</td>\n",
       "      <td>4.527640e+02</td>\n",
       "      <td>8.519444e+03</td>\n",
       "      <td>3.044565e+02</td>\n",
       "      <td>0.425998</td>\n",
       "      <td>0.028549</td>\n",
       "      <td>0.664118</td>\n",
       "      <td>3.595627</td>\n",
       "      <td>71586.980101</td>\n",
       "      <td>106494.270656</td>\n",
       "      <td>631.116457</td>\n",
       "      <td>4.024048</td>\n",
       "      <td>2160.753578</td>\n",
       "      <td>0.002888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.231757e+09</td>\n",
       "      <td>1.141255e+06</td>\n",
       "      <td>31.788936</td>\n",
       "      <td>4.237267e+05</td>\n",
       "      <td>77.706051</td>\n",
       "      <td>6.056537</td>\n",
       "      <td>2.003199</td>\n",
       "      <td>2.128168</td>\n",
       "      <td>2.192103</td>\n",
       "      <td>49.506785</td>\n",
       "      <td>1.020626</td>\n",
       "      <td>0.546063</td>\n",
       "      <td>0.447657</td>\n",
       "      <td>0.352275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109870</td>\n",
       "      <td>0.061601</td>\n",
       "      <td>1.679273e+05</td>\n",
       "      <td>6.864688e+03</td>\n",
       "      <td>1.130143e+05</td>\n",
       "      <td>4.205190e+03</td>\n",
       "      <td>0.494494</td>\n",
       "      <td>0.166535</td>\n",
       "      <td>0.472298</td>\n",
       "      <td>0.864574</td>\n",
       "      <td>39165.985756</td>\n",
       "      <td>7384.185494</td>\n",
       "      <td>1573.071335</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>3075.169206</td>\n",
       "      <td>0.053661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.147275e+09</td>\n",
       "      <td>2.407460e+05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.042610e+05</td>\n",
       "      <td>-115.172875</td>\n",
       "      <td>28.541290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>60556.000000</td>\n",
       "      <td>-9998.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.034005e+09</td>\n",
       "      <td>2.559520e+05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.267840e+05</td>\n",
       "      <td>-115.172875</td>\n",
       "      <td>36.114666</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.690000e+02</td>\n",
       "      <td>1.440000e+02</td>\n",
       "      <td>1.160000e+02</td>\n",
       "      <td>9.902000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>60039.000000</td>\n",
       "      <td>98238.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.193952e+07</td>\n",
       "      <td>2.777420e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.497539e+06</td>\n",
       "      <td>-73.986473</td>\n",
       "      <td>40.756680</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.390000e+02</td>\n",
       "      <td>2.730000e+02</td>\n",
       "      <td>2.350000e+02</td>\n",
       "      <td>1.890000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>95602.000000</td>\n",
       "      <td>109153.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>937.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.081646e+09</td>\n",
       "      <td>4.781330e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.504033e+06</td>\n",
       "      <td>-0.127804</td>\n",
       "      <td>41.880779</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.650000e+02</td>\n",
       "      <td>4.490000e+02</td>\n",
       "      <td>4.660000e+02</td>\n",
       "      <td>2.990000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>95656.000000</td>\n",
       "      <td>110287.000000</td>\n",
       "      <td>770.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.147266e+09</td>\n",
       "      <td>3.949856e+06</td>\n",
       "      <td>1082.000000</td>\n",
       "      <td>1.506246e+06</td>\n",
       "      <td>139.759995</td>\n",
       "      <td>51.507538</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.195489e+07</td>\n",
       "      <td>1.976733e+06</td>\n",
       "      <td>1.762130e+07</td>\n",
       "      <td>1.087117e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>116356.000000</td>\n",
       "      <td>116928.000000</td>\n",
       "      <td>19235.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>32399.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            srch_id      prop_key  srch_visitor_visit_nbr  \\\n",
       "count  8.411150e+05  8.411150e+05           841115.000000   \n",
       "mean   1.724233e+07  8.185569e+05               12.325977   \n",
       "std    1.231757e+09  1.141255e+06               31.788936   \n",
       "min   -2.147275e+09  2.407460e+05                1.000000   \n",
       "25%   -1.034005e+09  2.559520e+05                1.000000   \n",
       "50%    3.193952e+07  2.777420e+05                3.000000   \n",
       "75%    1.081646e+09  4.781330e+05               10.000000   \n",
       "max    2.147266e+09  3.949856e+06             1082.000000   \n",
       "\n",
       "       srch_hcom_destination_id  srch_dest_longitude  srch_dest_latitude  \\\n",
       "count              8.411150e+05        841115.000000       841115.000000   \n",
       "mean               1.194309e+06           -45.892803           40.445932   \n",
       "std                4.237267e+05            77.706051            6.056537   \n",
       "min                5.042610e+05          -115.172875           28.541290   \n",
       "25%                7.267840e+05          -115.172875           36.114666   \n",
       "50%                1.497539e+06           -73.986473           40.756680   \n",
       "75%                1.504033e+06            -0.127804           41.880779   \n",
       "max                1.506246e+06           139.759995           51.507538   \n",
       "\n",
       "         srch_ci_day    srch_co_day       srch_los        srch_bw  \\\n",
       "count  841115.000000  841115.000000  841115.000000  841115.000000   \n",
       "mean        4.272102       3.689650       2.677238      35.349157   \n",
       "std         2.003199       2.128168       2.192103      49.506785   \n",
       "min         1.000000       1.000000       1.000000       0.000000   \n",
       "25%         3.000000       2.000000       1.000000       4.000000   \n",
       "50%         5.000000       4.000000       2.000000      17.000000   \n",
       "75%         6.000000       6.000000       3.000000      46.000000   \n",
       "max         7.000000       7.000000      28.000000     473.000000   \n",
       "\n",
       "       srch_adults_cnt  srch_children_cnt    srch_rm_cnt  srch_mobile_bool  \\\n",
       "count    841097.000000      841097.000000  841115.000000     841115.000000   \n",
       "mean          2.019900           0.161467       1.113059          0.145173   \n",
       "std           1.020626           0.546063       0.447657          0.352275   \n",
       "min           1.000000           0.000000       1.000000          0.000000   \n",
       "25%           2.000000           0.000000       1.000000          0.000000   \n",
       "50%           2.000000           0.000000       1.000000          0.000000   \n",
       "75%           2.000000           0.000000       1.000000          0.000000   \n",
       "max          16.000000           8.000000       8.000000          1.000000   \n",
       "\n",
       "       srch_mobile_app  prop_travelad_bool  prop_dotd_bool  \\\n",
       "count         841115.0       841115.000000   841115.000000   \n",
       "mean               0.0            0.012221        0.003809   \n",
       "std                0.0            0.109870        0.061601   \n",
       "min                0.0            0.000000        0.000000   \n",
       "25%                0.0            0.000000        0.000000   \n",
       "50%                0.0            0.000000        0.000000   \n",
       "75%                0.0            0.000000        0.000000   \n",
       "max                0.0            1.000000        1.000000   \n",
       "\n",
       "       prop_price_without_discount_local  prop_price_without_discount_usd  \\\n",
       "count                       8.410640e+05                     8.410640e+05   \n",
       "mean                        1.204508e+04                     4.527640e+02   \n",
       "std                         1.679273e+05                     6.864688e+03   \n",
       "min                         8.000000e+00                     0.000000e+00   \n",
       "25%                         1.690000e+02                     1.440000e+02   \n",
       "50%                         3.390000e+02                     2.730000e+02   \n",
       "75%                         6.650000e+02                     4.490000e+02   \n",
       "max                         3.195489e+07                     1.976733e+06   \n",
       "\n",
       "       prop_price_with_discount_local  prop_price_with_discount_usd  \\\n",
       "count                    8.410640e+05                  8.410640e+05   \n",
       "mean                     8.519444e+03                  3.044565e+02   \n",
       "std                      1.130143e+05                  4.205190e+03   \n",
       "min                      6.000000e+00                  0.000000e+00   \n",
       "25%                      1.160000e+02                  9.902000e+01   \n",
       "50%                      2.350000e+02                  1.890000e+02   \n",
       "75%                      4.660000e+02                  2.990000e+02   \n",
       "max                      1.762130e+07                  1.087117e+06   \n",
       "\n",
       "        prop_imp_drr  prop_booking_bool  prop_brand_bool  prop_starrating  \\\n",
       "count  841115.000000      841115.000000    841115.000000    841115.000000   \n",
       "mean        0.425998           0.028549         0.664118         3.595627   \n",
       "std         0.494494           0.166535         0.472298         0.864574   \n",
       "min         0.000000           0.000000         0.000000         0.000000   \n",
       "25%         0.000000           0.000000         0.000000         3.000000   \n",
       "50%         0.000000           0.000000         1.000000         4.000000   \n",
       "75%         1.000000           0.000000         1.000000         4.000000   \n",
       "max         1.000000           1.000000         1.000000         5.000000   \n",
       "\n",
       "       prop_market_id  prop_submarket_id  prop_room_capacity  \\\n",
       "count   841115.000000      841115.000000       841115.000000   \n",
       "mean     71586.980101      106494.270656          631.116457   \n",
       "std      39165.985756        7384.185494         1573.071335   \n",
       "min        369.000000       60556.000000        -9998.000000   \n",
       "25%      60039.000000       98238.000000          144.000000   \n",
       "50%      95602.000000      109153.000000          306.000000   \n",
       "75%      95656.000000      110287.000000          770.000000   \n",
       "max     116356.000000      116928.000000        19235.000000   \n",
       "\n",
       "       prop_review_score  prop_review_count  prop_hostel_bool  \n",
       "count      841107.000000      841107.000000     841115.000000  \n",
       "mean            4.024048        2160.753578          0.002888  \n",
       "std             0.547170        3075.169206          0.053661  \n",
       "min             0.000000           0.000000          0.000000  \n",
       "25%             3.800000         306.000000          0.000000  \n",
       "50%             4.100000         937.000000          0.000000  \n",
       "75%             4.400000        2550.000000          0.000000  \n",
       "max             5.000000       32399.000000          1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numerical variables\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "1. prop_room_capacity column -- max number of rooms this property has available -- has negative values.  <br>\n",
    "2. The srch_mobile_app column is all zeros. <br>\n",
    "3. The prop_imp_drr is binary. It is not sure what does this binary variable mean. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_date_time</th>\n",
       "      <th>srch_visitor_id</th>\n",
       "      <th>srch_visitor_loc_country</th>\n",
       "      <th>srch_visitor_loc_region</th>\n",
       "      <th>srch_visitor_loc_city</th>\n",
       "      <th>srch_visitor_wr_member</th>\n",
       "      <th>srch_posa_continent</th>\n",
       "      <th>srch_posa_country</th>\n",
       "      <th>srch_ci</th>\n",
       "      <th>srch_co</th>\n",
       "      <th>srch_device</th>\n",
       "      <th>srch_currency</th>\n",
       "      <th>prop_super_region</th>\n",
       "      <th>prop_continent</th>\n",
       "      <th>prop_country</th>\n",
       "      <th>srch_local_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>841115</td>\n",
       "      <td>841115</td>\n",
       "      <td>841115</td>\n",
       "      <td>840992</td>\n",
       "      <td>841115</td>\n",
       "      <td>396237</td>\n",
       "      <td>355867</td>\n",
       "      <td>841115</td>\n",
       "      <td>841115</td>\n",
       "      <td>841115</td>\n",
       "      <td>841115</td>\n",
       "      <td>707011</td>\n",
       "      <td>841115</td>\n",
       "      <td>841115</td>\n",
       "      <td>841115</td>\n",
       "      <td>841115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>23873</td>\n",
       "      <td>23101</td>\n",
       "      <td>151</td>\n",
       "      <td>599</td>\n",
       "      <td>5092</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>385</td>\n",
       "      <td>376</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2014-09-22 23:33:00</td>\n",
       "      <td>0fac9b35-d7b7-4b71-b275-7da40310ea2e</td>\n",
       "      <td>UNITED STATES OF AMERICA</td>\n",
       "      <td>CA</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>Not Signed In|Returning Visitor|Not FC Member</td>\n",
       "      <td>EUROPE</td>\n",
       "      <td>US</td>\n",
       "      <td>2014-09-26</td>\n",
       "      <td>2014-09-28</td>\n",
       "      <td>DESKTOP</td>\n",
       "      <td>USD</td>\n",
       "      <td>AMER</td>\n",
       "      <td>NORTHAMERICA</td>\n",
       "      <td>UNITED STATES OF AMERICA</td>\n",
       "      <td>2014-09-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>106</td>\n",
       "      <td>800</td>\n",
       "      <td>463183</td>\n",
       "      <td>115310</td>\n",
       "      <td>21172</td>\n",
       "      <td>186401</td>\n",
       "      <td>207181</td>\n",
       "      <td>463532</td>\n",
       "      <td>23604</td>\n",
       "      <td>28497</td>\n",
       "      <td>718871</td>\n",
       "      <td>412926</td>\n",
       "      <td>554304</td>\n",
       "      <td>554304</td>\n",
       "      <td>554304</td>\n",
       "      <td>39487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             srch_date_time                       srch_visitor_id  \\\n",
       "count                841115                                841115   \n",
       "unique                23873                                 23101   \n",
       "top     2014-09-22 23:33:00  0fac9b35-d7b7-4b71-b275-7da40310ea2e   \n",
       "freq                    106                                   800   \n",
       "\n",
       "        srch_visitor_loc_country srch_visitor_loc_region  \\\n",
       "count                     841115                  840992   \n",
       "unique                       151                     599   \n",
       "top     UNITED STATES OF AMERICA                      CA   \n",
       "freq                      463183                  115310   \n",
       "\n",
       "       srch_visitor_loc_city                         srch_visitor_wr_member  \\\n",
       "count                 841115                                         396237   \n",
       "unique                  5092                                              9   \n",
       "top                 NEW YORK  Not Signed In|Returning Visitor|Not FC Member   \n",
       "freq                   21172                                         186401   \n",
       "\n",
       "       srch_posa_continent srch_posa_country     srch_ci     srch_co  \\\n",
       "count               355867            841115      841115      841115   \n",
       "unique                   4                67         385         376   \n",
       "top                 EUROPE                US  2014-09-26  2014-09-28   \n",
       "freq                207181            463532       23604       28497   \n",
       "\n",
       "       srch_device srch_currency prop_super_region prop_continent  \\\n",
       "count       841115        707011            841115         841115   \n",
       "unique           3            51                 4              4   \n",
       "top        DESKTOP           USD              AMER   NORTHAMERICA   \n",
       "freq        718871        412926            554304         554304   \n",
       "\n",
       "                    prop_country srch_local_date  \n",
       "count                     841115          841115  \n",
       "unique                         7              28  \n",
       "top     UNITED STATES OF AMERICA      2014-09-03  \n",
       "freq                      554304           39487  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categorical variables\n",
    "train.describe(exclude=[np.number]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate negative property room capcity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows with negative room capacity in the training set: 10401\n",
      "rows with negative room capacity in the test set: 4438\n"
     ]
    }
   ],
   "source": [
    "print(\"rows with negative room capacity in the training set: \" + str(sum(train.prop_room_capacity < 0)))\n",
    "print(\"rows with negative room capacity in the test set: \" + str(sum(test.prop_room_capacity < 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>prop_key</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_room_capacity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-9998</th>\n",
       "      <td>6978</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1151</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>177</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    srch_id  prop_key\n",
       "prop_room_capacity                   \n",
       "-9998                  6978       103\n",
       " 0                     1151         3\n",
       " 1                      177        98\n",
       " 2                       62        24\n",
       " 3                      139        49"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('prop_room_capacity')['srch_id', 'prop_key'].nunique().head()\n",
    "#test.groupby('prop_room_capacity')['srch_id', 'prop_key'].nunique().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All negative values are -9998. It is likely an indication that the data is not available. In the training set, 103 properties have -9998 room capacities, correpsonding to 10401 rows (1%) and 6978 search Ids.\n",
    "#### Check if properties with negative capacity have any nonnegative records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties with negative capacity data do not have other records: \n",
      "True\n",
      "Do these properties exist in the test set: \n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# all records with negative capacities\n",
    "neg_capacity_row = train[train.prop_room_capacity < 0]\n",
    "neg_capacity_property = set(neg_capacity_row.prop_key)\n",
    "count_neg_capacity_property = neg_capacity_row.groupby('prop_key')['srch_id'].count()\n",
    "\n",
    "# all records from properties that have negative capacity \n",
    "neg_capacity_property_all_records = train[train.prop_key.isin(neg_capacity_property)]\n",
    "count_neg_capacity_property_all_records = neg_capacity_property_all_records.groupby('prop_key')['srch_id'].count()\n",
    "\n",
    "# check if total number of records per property match\n",
    "print(\"Properties with negative capacity data do not have other records: \")\n",
    "print(count_neg_capacity_property.equals(count_neg_capacity_property_all_records))\n",
    "\n",
    "# check if these properties exist in the test set\n",
    "print(\"Do these properties exist in the test set: \")\n",
    "print(any(test.prop_key.isin(neg_capacity_property)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implications\n",
    "1. Properties without room capacity info should be given a low rank because we are not sure if they have enough capacities to accomodate customers' needs.\n",
    "2. Consider removing srch_mobile_app and prop_imp_drr columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use correlations to check if the property price is per night per room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlations between price and length of stay: -0.00579965687736116\n",
      "correlations between price and room requested: -0.001780664434088739\n"
     ]
    }
   ],
   "source": [
    "print(\"correlations between price and length of stay: \" + str(train.srch_los.corr(train.prop_price_with_discount_usd)))\n",
    "print(\"correlations between price and room requested: \" + str(train.srch_rm_cnt.corr(train.prop_price_with_discount_usd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the correlations between price and the length of day and between price and room requested are small and negative, it is likely that the price is per night and per room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check frequency of number of search queries in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  373.,  7785.,   123.,   143.,   215.,   317.,   690.,   617.,\n",
       "         1237., 12513.]),\n",
       " array([ 1. ,  6.3, 11.6, 16.9, 22.2, 27.5, 32.8, 38.1, 43.4, 48.7, 54. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff755815400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counter = Counter(train.srch_id)\n",
    "counter.most_common()[:-10-1:-1]\n",
    "plt.hist(list(counter.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Process the Data: Latent Factor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_imputation_map(df):\n",
    "    na_columns = ['prop_price_with_discount_usd', 'prop_review_count', \n",
    "              'prop_review_score', 'srch_adults_cnt', 'srch_children_cnt']\n",
    "    impute_dict = {}\n",
    "    \n",
    "    for column in na_columns:\n",
    "        column_mean = np.nanmean(df[column], axis=0)\n",
    "        impute_dict[column] = column_mean\n",
    "\n",
    "    return impute_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(df, impute_dict):\n",
    "    # remove columns that do not have enough datapoints or do not provide useful info\n",
    "    new_df = df.drop(['srch_visitor_wr_member', \n",
    "                 'srch_visitor_loc_region',\n",
    "                 'srch_posa_continent',\n",
    "                 'srch_currency',\n",
    "                 'srch_mobile_app',\n",
    "                 'prop_imp_drr'], axis=1)\n",
    "    \n",
    "    # create a column of month for a particular search\n",
    "    new_df['srch_local_month'] = pd.to_datetime(new_df['srch_local_date']).dt.month\n",
    "    new_df['srch_checkin_month'] = pd.to_datetime(new_df['srch_ci']).dt.month\n",
    "    new_df['srch_checkin_day'] = pd.to_datetime(new_df['srch_ci']).dt.day\n",
    "    \n",
    "    # impute columns with missing values     \n",
    "    new_df.fillna(value = impute_dict, inplace=True)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "impute_dict = create_imputation_map(train)\n",
    "train_processed = preprocessing(train, impute_dict)\n",
    "test_processed = preprocessing(test, impute_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train-val split, randomly select 10% of the user for validation\n",
    "def split_train_val(df):\n",
    "    user_list = df.srch_visitor_id.unique()\n",
    "    m = len(user_list)\n",
    "    new_user_list = deepcopy(user_list)\n",
    "    random.shuffle(new_user_list)\n",
    "    \n",
    "    val_users_id = new_user_list[: int(0.1*m)]\n",
    "    \n",
    "    train_df = df[~df.srch_visitor_id.isin(val_users_id)]\n",
    "    val_df = df[df.srch_visitor_id.isin(val_users_id)]\n",
    "    \n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_train, final_val = split_train_val(train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save processed dataframe \n",
    "train_processed_path = os.path.join(OUTPUT_PATH, \"train_splited.pickle\")\n",
    "val_processed_path = os.path.join(OUTPUT_PATH, \"val_splited.pickle\")\n",
    "#test_processed_path = os.path.join(OUTPUT_PATH, \"test_processed.pickle\")\n",
    "\n",
    "with open(train_processed_path, 'wb') as trainOutput:\n",
    "    pickle.dump(final_train, trainOutput)\n",
    "\n",
    "with open(val_processed_path, 'wb') as valOutput:\n",
    "    pickle.dump(final_val, valOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20791\n"
     ]
    }
   ],
   "source": [
    "print(final_train.srch_visitor_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create user-item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of properties present in the train and test set\n",
    "all_properties = list(set(train.prop_key.unique()).union(set(test.prop_key.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create user-property booking matrix\n",
    "def create_user_property_matrix(df, properties_set):\n",
    "    properties = np.sort(properties_set)\n",
    "    users = np.sort(df.srch_visitor_id.unique())\n",
    "\n",
    "    # map unique properties and users to index values\n",
    "    properties_with_index = pd.DataFrame({'index_p':range(0,len(properties)), 'properties':properties})\n",
    "    users_with_index = pd.DataFrame({'index_u':range(0,len(users)), 'users':users})\n",
    "\n",
    "    # join dataframe with property and user index\n",
    "    df_add_properties_index = df.join(properties_with_index.set_index('properties'), on='prop_key')\n",
    "    df_add_users_properties_index = df_add_properties_index.join(users_with_index.set_index('users'), on='srch_visitor_id')\n",
    "    \n",
    "    # get the number of bookings per property-user pair\n",
    "    num_bookings_df = df_add_users_properties_index.groupby(['index_p', 'index_u'])['prop_booking_bool'].sum().reset_index(name='num_booking')\n",
    "    num_bookings_nonzero_df = num_bookings_df[num_bookings_df.num_booking != 0]\n",
    "    \n",
    "    booking_matrix = np.zeros((len(users), len(properties)))\n",
    "    \n",
    "    property_index = np.array(num_bookings_nonzero_df.index_p)\n",
    "    user_index = np.array(num_bookings_nonzero_df.index_u)\n",
    "    num_bookings = np.array(num_bookings_nonzero_df.num_booking)\n",
    "    booking_matrix[user_index, property_index] = num_bookings\n",
    "    \n",
    "    return properties_with_index, users_with_index, booking_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_idx_mapping, users_idx_mapping, user_property_booking_matrix = create_user_property_matrix(final_train, all_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save index mapping and user_item_matrix \n",
    "properties_idx_mapping_path = os.path.join('output', 'train_splitted_properties_idx_mapping.pickle')\n",
    "users_idx_mapping_path = os.path.join('output', 'orig_train_splitted_users_idx_mapping.pickle')\n",
    "user_property_booking_matrix_path = os.path.join('output', 'train_splitted_user_property_matrix.pickle')\n",
    "\n",
    "with open(properties_idx_mapping_path, 'wb') as output_prop:\n",
    "    pickle.dump(properties_idx_mapping, output_prop)\n",
    "\n",
    "with open(users_idx_mapping_path, 'wb') as output_user:\n",
    "    pickle.dump(users_idx_mapping, output_user)\n",
    "    \n",
    "with open(user_property_booking_matrix_path, 'wb') as output_matrix:\n",
    "    pickle.dump(user_property_booking_matrix, output_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define user and property parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create user features df\n",
    "user_parameters = ['srch_visitor_visit_nbr',\n",
    "                   'srch_dest_longitude',\n",
    "                   'srch_dest_latitude',\n",
    "                   'srch_checkin_month',\n",
    "                   'srch_checkin_day',\n",
    "                   'srch_ci_day', \n",
    "                   'srch_los', \n",
    "                   'srch_bw',\n",
    "                   'srch_adults_cnt', \n",
    "                   'srch_children_cnt', \n",
    "                   'srch_rm_cnt', \n",
    "                   'srch_mobile_bool',\n",
    "                   'srch_local_month'\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit feedback recommendation using logistic latent Factor model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticMF():\n",
    "\n",
    "    def __init__(self, counts, num_factors, iterations, reg_param=0.6, gamma=1.0):\n",
    "        self.counts = counts\n",
    "        self.num_users = counts.shape[0]\n",
    "        self.num_items = counts.shape[1]\n",
    "        self.num_factors = num_factors\n",
    "        self.iterations = iterations\n",
    "        self.reg_param = reg_param\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        self.ones = np.ones((self.num_users, self.num_items))\n",
    "        self.user_vectors = np.random.normal(size=(self.num_users, self.num_factors))\n",
    "        self.item_vectors = np.random.normal(size=(self.num_items, self.num_factors))\n",
    "        self.user_biases = np.random.normal(size=(self.num_users, 1))\n",
    "        self.item_biases = np.random.normal(size=(self.num_items, 1))\n",
    "\n",
    "        user_vec_deriv_sum = np.zeros((self.num_users, self.num_factors))\n",
    "        item_vec_deriv_sum = np.zeros((self.num_items, self.num_factors))\n",
    "        user_bias_deriv_sum = np.zeros((self.num_users, 1))\n",
    "        item_bias_deriv_sum = np.zeros((self.num_items, 1))\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            t0 = time.time()\n",
    "            # Fix items and solve for users\n",
    "            # take step towards gradient of deriv of log likelihood\n",
    "            # we take a step in positive direction because we are maximizing LL\n",
    "            user_vec_deriv, user_bias_deriv = self.deriv(True)\n",
    "            user_vec_deriv_sum += np.square(user_vec_deriv)\n",
    "            user_bias_deriv_sum += np.square(user_bias_deriv)\n",
    "            vec_step_size = self.gamma / np.sqrt(user_vec_deriv_sum)\n",
    "            bias_step_size = self.gamma / np.sqrt(user_bias_deriv_sum)\n",
    "            self.user_vectors += vec_step_size * user_vec_deriv\n",
    "            self.user_biases += bias_step_size * user_bias_deriv\n",
    "\n",
    "            # Fix users and solve for items\n",
    "            # take step towards gradient of deriv of log likelihood\n",
    "            # we take a step in positive direction because we are maximizing LL\n",
    "            item_vec_deriv, item_bias_deriv = self.deriv(False)\n",
    "            item_vec_deriv_sum += np.square(item_vec_deriv)\n",
    "            item_bias_deriv_sum += np.square(item_bias_deriv)\n",
    "            vec_step_size = self.gamma / np.sqrt(item_vec_deriv_sum)\n",
    "            bias_step_size = self.gamma / np.sqrt(item_bias_deriv_sum)\n",
    "            self.item_vectors += vec_step_size * item_vec_deriv\n",
    "            self.item_biases += bias_step_size * item_bias_deriv\n",
    "            t1 = time.time()\n",
    "\n",
    "            print('iteration %i finished in %f seconds' % (i + 1, t1 - t0))\n",
    "\n",
    "    def deriv(self, user):\n",
    "        if user:\n",
    "            vec_deriv = np.dot(self.counts, self.item_vectors)\n",
    "            bias_deriv = np.expand_dims(np.sum(self.counts, axis=1), 1)\n",
    "\n",
    "        else:\n",
    "            vec_deriv = np.dot(self.counts.T, self.user_vectors)\n",
    "            bias_deriv = np.expand_dims(np.sum(self.counts, axis=0), 1)\n",
    "            \n",
    "        A = np.dot(self.user_vectors, self.item_vectors.T)\n",
    "        A += self.user_biases\n",
    "        A += self.item_biases.T\n",
    "        A = np.exp(A)\n",
    "        A /= (A + self.ones)\n",
    "        A = (self.counts + self.ones) * A\n",
    "\n",
    "        if user:\n",
    "            vec_deriv -= np.dot(A, self.item_vectors)\n",
    "            bias_deriv -= np.expand_dims(np.sum(A, axis=1), 1)\n",
    "            # L2 regularization\n",
    "            vec_deriv -= self.reg_param * self.user_vectors\n",
    "        else:\n",
    "            vec_deriv -= np.dot(A.T, self.user_vectors)\n",
    "            bias_deriv -= np.expand_dims(np.sum(A, axis=0), 1)\n",
    "            # L2 regularization\n",
    "            vec_deriv -= self.reg_param * self.item_vectors\n",
    "            \n",
    "        return (vec_deriv, bias_deriv)\n",
    "\n",
    "    \n",
    "    def log_likelihood(self):\n",
    "        loglik = 0\n",
    "        A = np.dot(self.user_vectors, self.item_vectors.T)\n",
    "        A += self.user_biases\n",
    "        A += self.item_biases.T\n",
    "        B = A * self.counts\n",
    "        loglik += np.sum(B)\n",
    "\n",
    "        A = np.exp(A)\n",
    "        A += self.ones\n",
    "\n",
    "        A = np.log(A)\n",
    "        A = (self.counts + self.ones) * A\n",
    "        loglik -= np.sum(A)\n",
    "\n",
    "        # L2 regularization\n",
    "        loglik -= 0.5 * self.reg_param * np.sum(np.square(self.user_vectors))\n",
    "        loglik -= 0.5 * self.reg_param * np.sum(np.square(self.item_vectors))\n",
    "        return loglik\n",
    "\n",
    "    \n",
    "    def print_vectors(self):\n",
    "        user_vecs_file = open('logmf-user-vecs-%i' % self.num_factors, 'w')\n",
    "        for i in range(self.num_users):\n",
    "            vec = ' '.join(map(str, self.user_vectors[i]))\n",
    "            line = '%i\\t%s\\n' % (i, vec)\n",
    "            user_vecs_file.write(line)\n",
    "        user_vecs_file.close()\n",
    "        item_vecs_file = open('logmf-item-vecs-%i' % self.num_factors, 'w')\n",
    "        for i in range(self.num_items):\n",
    "            vec = ' '.join(map(str, self.item_vectors[i]))\n",
    "            line = '%i\\t%s\\n' % (i, vec)\n",
    "            item_vecs_file.write(line)\n",
    "        item_vecs_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model, train the model, and get user and item latent vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(num_latent_factors, iterations, model_number, u_p_matrix):\n",
    "    \n",
    "    logistic_latent_factor_model = LogisticMF(u_p_matrix, num_latent_factors, iterations)\n",
    "    \n",
    "    logistic_latent_factor_model.train_model()\n",
    "    \n",
    "    user_vector_matrix = logistic_latent_factor_model.user_vectors\n",
    "    item_vector_matrix = logistic_latent_factor_model.item_vectors\n",
    "    \n",
    "    # save results\n",
    "    u_vector_file_name = os.path.join(OUTPUT_PATH, \"train_splitted_\" + str(model_number) + \"_user_vector_matrix.pickle\")\n",
    "    i_vector_file_name = os.path.join(OUTPUT_PATH, \"train_splitted_\" + str(model_number) + \"_item_vector_matrix.pickle\")\n",
    "\n",
    "    with open(u_vector_file_name, 'wb') as userMatrix:\n",
    "        pickle.dump(user_vector_matrix, userMatrix)\n",
    "\n",
    "    with open(i_vector_file_name, 'wb') as itemMatrix:\n",
    "        pickle.dump(item_vector_matrix, itemMatrix)\n",
    "        \n",
    "    return user_vector_matrix, item_vector_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent factor model 1: 100 latent factors, 60 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 finished in 9.000347 seconds\n",
      "iteration 2 finished in 8.343534 seconds\n",
      "iteration 3 finished in 8.295838 seconds\n",
      "iteration 4 finished in 8.486530 seconds\n",
      "iteration 5 finished in 8.576141 seconds\n",
      "iteration 6 finished in 8.449510 seconds\n",
      "iteration 7 finished in 8.515248 seconds\n",
      "iteration 8 finished in 8.170882 seconds\n",
      "iteration 9 finished in 8.341069 seconds\n",
      "iteration 10 finished in 8.367486 seconds\n",
      "iteration 11 finished in 8.672386 seconds\n",
      "iteration 12 finished in 8.268026 seconds\n",
      "iteration 13 finished in 8.781662 seconds\n",
      "iteration 14 finished in 8.632397 seconds\n",
      "iteration 15 finished in 8.706172 seconds\n",
      "iteration 16 finished in 8.294159 seconds\n",
      "iteration 17 finished in 8.653640 seconds\n",
      "iteration 18 finished in 8.588908 seconds\n",
      "iteration 19 finished in 8.654991 seconds\n",
      "iteration 20 finished in 8.915171 seconds\n",
      "iteration 21 finished in 8.830005 seconds\n",
      "iteration 22 finished in 8.807927 seconds\n",
      "iteration 23 finished in 8.661125 seconds\n",
      "iteration 24 finished in 8.525918 seconds\n",
      "iteration 25 finished in 8.902601 seconds\n",
      "iteration 26 finished in 8.672720 seconds\n",
      "iteration 27 finished in 8.669923 seconds\n",
      "iteration 28 finished in 8.877639 seconds\n",
      "iteration 29 finished in 8.756807 seconds\n",
      "iteration 30 finished in 8.520211 seconds\n",
      "iteration 31 finished in 8.940369 seconds\n",
      "iteration 32 finished in 9.610836 seconds\n",
      "iteration 33 finished in 9.302665 seconds\n",
      "iteration 34 finished in 9.207455 seconds\n",
      "iteration 35 finished in 8.642220 seconds\n",
      "iteration 36 finished in 8.639686 seconds\n",
      "iteration 37 finished in 8.707722 seconds\n",
      "iteration 38 finished in 8.640985 seconds\n",
      "iteration 39 finished in 8.687565 seconds\n",
      "iteration 40 finished in 8.824171 seconds\n",
      "iteration 41 finished in 8.915413 seconds\n",
      "iteration 42 finished in 8.756021 seconds\n",
      "iteration 43 finished in 8.775344 seconds\n",
      "iteration 44 finished in 8.376763 seconds\n",
      "iteration 45 finished in 8.484011 seconds\n",
      "iteration 46 finished in 8.554251 seconds\n",
      "iteration 47 finished in 8.694339 seconds\n",
      "iteration 48 finished in 8.661509 seconds\n",
      "iteration 49 finished in 8.700852 seconds\n",
      "iteration 50 finished in 8.593590 seconds\n",
      "iteration 51 finished in 8.656771 seconds\n",
      "iteration 52 finished in 8.664148 seconds\n",
      "iteration 53 finished in 8.839215 seconds\n",
      "iteration 54 finished in 8.810981 seconds\n",
      "iteration 55 finished in 8.640616 seconds\n",
      "iteration 56 finished in 8.489483 seconds\n",
      "iteration 57 finished in 8.919369 seconds\n",
      "iteration 58 finished in 8.824056 seconds\n",
      "iteration 59 finished in 8.864463 seconds\n",
      "iteration 60 finished in 8.859923 seconds\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "user_vector_matrix, item_vector_matrix = train_model(100, 60, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return top k similar visitors (most similar search queries) and their similarity score\n",
    "# distance metric: cosine distance\n",
    "def find_similar_user_index(target_row, train_df_unique_srchId, k_users):\n",
    "    \n",
    "    visitor_id_list = train_df_unique_srchId.srch_visitor_id\n",
    "    train_df_user_parameters = np.array(train_df_unique_srchId[user_parameters])\n",
    "    \n",
    "    cosine_distance = cosine_similarity(target_row, train_df_user_parameters).reshape(train_df_user_parameters.shape[0],)\n",
    "    \n",
    "    srch_similarity_df = pd.DataFrame({'srch_visitor_id': visitor_id_list,\n",
    "                                       'similarity': cosine_distance})\n",
    "    \n",
    "    top_k_similarity = srch_similarity_df.sort_values(by=['similarity'], ascending=False).head(k_users)\n",
    "    \n",
    "    return top_k_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find corresponding rows for k similar visitors in the user vector matrix\n",
    "def find_rows_in_user_vector(top_k_df, user_vector_matrix):\n",
    "\n",
    "    top_k_similar_users_df = top_k_df.join(users_idx_mapping.set_index('users'), on='srch_visitor_id')\n",
    "    top_k_user_vectors_rows = user_vector_matrix[top_k_similar_users_df.index_u]\n",
    "    \n",
    "    return top_k_user_vectors_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find corresponding row for the property in the item vector matrix\n",
    "def find_target_property_in_item_vectors(prop_id, item_vector_matrix):\n",
    "    prop_index = properties_idx_mapping[properties_idx_mapping['properties'] == prop_id].index_p.item()\n",
    "    \n",
    "    return item_vector_matrix[prop_index,:].reshape(item_vector_matrix.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute preference of k visitors\n",
    "def compute_preference_k_users(user_vector_rows, item_vector_column):\n",
    "    return np.dot(user_vector_rows, item_vector_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute the overall preference weighted by similarity\n",
    "def compute_score(preference, similarity):\n",
    "    return np.sum(np.multiply(preference, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_path = os.path.join(OUTPUT_PATH, \"test_pred_unordered_latent_factor_\" + \"1\" + \".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_scores_for_df(train_df, test_df, user_parameters, user_vector_matrix, \n",
    "                          item_vector_matrix, save_path, k_users):\n",
    "    train_df_unique_srch_id = train_df.drop_duplicates(subset=['srch_id'])\n",
    "    #train_user_parameters_array = np.array(train_df[user_parameters])\n",
    "    result_df = pd.DataFrame({'srch_id':test_df.srch_id, \n",
    "                              'prop_key':test_df.prop_key, \n",
    "                              'score':np.zeros(test_df.shape[0])})\n",
    "    \n",
    "    def compute_score_update_row(target_row, row_index):\n",
    "        target_srch_row = pd.DataFrame(target_row).transpose()\n",
    "        target_srch_query = target_srch_row[user_parameters]\n",
    "        \n",
    "        top_k_similar_users_idx = find_similar_user_index(np.array(target_srch_query), \\\n",
    "                                                          train_df_unique_srch_id, k_users)\n",
    "        user_vector_rows = find_rows_in_user_vector(top_k_similar_users_idx, user_vector_matrix)\n",
    "        item_vector_row = find_target_property_in_item_vectors(target_srch_row.prop_key.item(), item_vector_matrix)\n",
    "        \n",
    "        preference_for_each_k_user = compute_preference_k_users(user_vector_rows, item_vector_row)\n",
    "        result = compute_score(preference_for_each_k_user, np.array(top_k_similar_users_idx.similarity).reshape(k_users,1))\n",
    "        result_df.loc[row_index, 'score'] = result\n",
    "\n",
    "    for idx, row in notebook.tqdm(test_df.iterrows()):\n",
    "        compute_score_update_row(row, idx)\n",
    "        \n",
    "    with open(save_path, 'wb') as output_result:\n",
    "        pickle.dump(result_df, output_result)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction for the validation set: weighted preference from 5 most similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bdac63eab841cfab290ce474594ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = compute_scores_for_df(final_train, final_val, user_parameters, user_vector_matrix, item_vector_matrix, k_users=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_ordered = results.sort_values(by=['srch_id', 'score'], ascending = [True, False])\n",
    "results_ordered_drop_score = results_ordered.drop(['score'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_ordered_drop_score.to_csv('output/logit_latent_factor_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric: Percentage of users that booked the property in the top 5 list recommended in the latent factor model\n",
    "#### DCG for each user @ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_DCG_at_top_k(booked_prop, recom_prop, rank_k):\n",
    "    booked_set = set(booked_prop)\n",
    "    recom_prop_top_k = recom_prop[:min(rank_k, len(recom_prop))]\n",
    "    index_position = [i for i, e in enumerate(recom_prop_top_k) if e in booked_set]\n",
    "    dcg = 0\n",
    "    for index in index_position:\n",
    "        dcg += 1/math.log(index+2, 2)     \n",
    "    return dcg\n",
    "\n",
    "def calc_dcg(res, val_info, rank_k):\n",
    "    top_k_recommendation = res.groupby('srch_visitor_id')['prop_key'].apply(list).reset_index(name='recom_prop')\n",
    "    top_k_recommendation_with_booked_prop = top_k_recommendation\\\n",
    "                                            .join(val_info.set_index('srch_visitor_id'), \n",
    "                                                  on ='srch_visitor_id',\n",
    "                                                  how = 'inner')\n",
    "        \n",
    "    top_k_recommendation_with_booked_prop['index'] = top_k_recommendation_with_booked_prop\\\n",
    "                                                    .apply(lambda row : calculate_DCG_at_top_k(row['booked_prop'], \n",
    "                                                                                               row['recom_prop'],\n",
    "                                                                                               rank_k), \n",
    "                                                           axis = 1)   \n",
    "        \n",
    "    return top_k_recommendation_with_booked_prop     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join result with their user_id\n",
    "res_with_visitor_id = results_ordered_drop_score.join(final_val[['srch_id', 'srch_visitor_id']]\\\n",
    "                                                      .set_index('srch_id'), on = 'srch_id')\n",
    "\n",
    "# get booking records for visitors in the validation set\n",
    "user_with_booked_prop = final_val[final_val.prop_booking_bool == 1][['srch_id', 'srch_visitor_id', 'prop_key']]\n",
    "booked_prop_groupby_user = user_with_booked_prop.groupby('srch_visitor_id')['prop_key'].apply(list).reset_index(name='booked_prop')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcg_for_each_user = calc_dcg(res_with_visitor_id, booked_prop_groupby_user, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcg@10: 0.4318624145110424\n"
     ]
    }
   ],
   "source": [
    "dcg_all_users = np.mean(dcg_for_each_user[['index']])\n",
    "print(\"dcg@10: \" + str(dcg_all_users.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put model training and evaluation functions together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def latent_factor_model_training(num_latent_factors, iterations, model_number, user_property_matrix):\n",
    "    user_vector_matrix, item_vector_matrix = train_model(num_latent_factors, iterations, model_number, user_property_matrix)\n",
    "    \n",
    "    results_path = os.path.join(OUTPUT_PATH, \"final_tuned_pred_unordered_latent_factor_\" + str(model_number) + \".pickle\")\n",
    "    \n",
    "    #results = compute_scores_for_df(final_train, final_val, user_parameters, user_vector_matrix, item_vector_matrix, results_path, k_users=5)\n",
    "    results = compute_scores_for_df(train_processed, test_processed, user_parameters, user_vector_matrix, item_vector_matrix, results_path, k_users=5)\n",
    "    \n",
    "    results_ordered = results.sort_values(by=['srch_id', 'score'], ascending = [True, False])\n",
    "    #results_ordered_drop_score = results_ordered.drop(['score'], axis=1)\n",
    "    \n",
    "    return results_ordered\n",
    "\n",
    "\n",
    "def latent_factor_model_evaluation(ordered_results, val_set, val_booking_info):\n",
    "    \n",
    "    res_with_visitor_id = ordered_results.join(val_set[['srch_id', 'srch_visitor_id']]\\\n",
    "                                                      .set_index('srch_id'), on = 'srch_id')\n",
    "\n",
    "    dcg_for_each_user = calc_dcg(res_with_visitor_id, val_booking_info, rank_k=10)\n",
    "    \n",
    "    dcg_all_users = np.mean(dcg_for_each_user[['index']])\n",
    "    #print(\"dcg@10: \" + str(dcg_all_users.item()))\n",
    "    \n",
    "    return dcg_all_users.item()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent factor model 2: 20 latent factors, 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 finished in 8.308824 seconds\n",
      "iteration 2 finished in 8.098759 seconds\n",
      "iteration 3 finished in 8.051920 seconds\n",
      "iteration 4 finished in 8.225375 seconds\n",
      "iteration 5 finished in 7.821528 seconds\n",
      "iteration 6 finished in 7.719490 seconds\n",
      "iteration 7 finished in 7.799818 seconds\n",
      "iteration 8 finished in 7.829719 seconds\n",
      "iteration 9 finished in 7.905992 seconds\n",
      "iteration 10 finished in 7.978943 seconds\n",
      "iteration 11 finished in 8.086609 seconds\n",
      "iteration 12 finished in 8.233547 seconds\n",
      "iteration 13 finished in 8.116977 seconds\n",
      "iteration 14 finished in 8.055948 seconds\n",
      "iteration 15 finished in 7.990087 seconds\n",
      "iteration 16 finished in 8.081241 seconds\n",
      "iteration 17 finished in 8.027063 seconds\n",
      "iteration 18 finished in 8.005428 seconds\n",
      "iteration 19 finished in 7.847799 seconds\n",
      "iteration 20 finished in 8.382077 seconds\n",
      "iteration 21 finished in 8.005615 seconds\n",
      "iteration 22 finished in 7.711496 seconds\n",
      "iteration 23 finished in 7.651276 seconds\n",
      "iteration 24 finished in 8.851140 seconds\n",
      "iteration 25 finished in 8.595048 seconds\n",
      "iteration 26 finished in 8.190055 seconds\n",
      "iteration 27 finished in 8.111555 seconds\n",
      "iteration 28 finished in 8.077952 seconds\n",
      "iteration 29 finished in 8.042497 seconds\n",
      "iteration 30 finished in 8.069106 seconds\n",
      "iteration 31 finished in 7.964353 seconds\n",
      "iteration 32 finished in 8.057223 seconds\n",
      "iteration 33 finished in 8.019898 seconds\n",
      "iteration 34 finished in 8.018086 seconds\n",
      "iteration 35 finished in 7.958936 seconds\n",
      "iteration 36 finished in 8.064605 seconds\n",
      "iteration 37 finished in 8.227343 seconds\n",
      "iteration 38 finished in 7.964699 seconds\n",
      "iteration 39 finished in 8.220979 seconds\n",
      "iteration 40 finished in 8.437648 seconds\n",
      "iteration 41 finished in 8.635880 seconds\n",
      "iteration 42 finished in 7.589889 seconds\n",
      "iteration 43 finished in 7.558640 seconds\n",
      "iteration 44 finished in 7.646261 seconds\n",
      "iteration 45 finished in 7.616903 seconds\n",
      "iteration 46 finished in 7.546536 seconds\n",
      "iteration 47 finished in 7.564378 seconds\n",
      "iteration 48 finished in 7.456324 seconds\n",
      "iteration 49 finished in 7.522671 seconds\n",
      "iteration 50 finished in 7.418991 seconds\n",
      "iteration 51 finished in 7.486369 seconds\n",
      "iteration 52 finished in 7.481458 seconds\n",
      "iteration 53 finished in 7.465694 seconds\n",
      "iteration 54 finished in 7.458800 seconds\n",
      "iteration 55 finished in 7.451783 seconds\n",
      "iteration 56 finished in 7.459020 seconds\n",
      "iteration 57 finished in 7.488730 seconds\n",
      "iteration 58 finished in 7.509849 seconds\n",
      "iteration 59 finished in 7.442525 seconds\n",
      "iteration 60 finished in 7.453978 seconds\n",
      "iteration 61 finished in 7.404637 seconds\n",
      "iteration 62 finished in 7.436880 seconds\n",
      "iteration 63 finished in 7.484041 seconds\n",
      "iteration 64 finished in 7.457401 seconds\n",
      "iteration 65 finished in 7.450111 seconds\n",
      "iteration 66 finished in 7.445337 seconds\n",
      "iteration 67 finished in 7.469053 seconds\n",
      "iteration 68 finished in 7.429941 seconds\n",
      "iteration 69 finished in 7.424183 seconds\n",
      "iteration 70 finished in 7.453498 seconds\n",
      "iteration 71 finished in 7.482696 seconds\n",
      "iteration 72 finished in 7.504369 seconds\n",
      "iteration 73 finished in 7.471421 seconds\n",
      "iteration 74 finished in 7.648488 seconds\n",
      "iteration 75 finished in 7.505431 seconds\n",
      "iteration 76 finished in 7.544168 seconds\n",
      "iteration 77 finished in 7.562340 seconds\n",
      "iteration 78 finished in 7.491864 seconds\n",
      "iteration 79 finished in 7.575376 seconds\n",
      "iteration 80 finished in 7.609547 seconds\n",
      "iteration 81 finished in 7.752647 seconds\n",
      "iteration 82 finished in 7.594172 seconds\n",
      "iteration 83 finished in 7.545402 seconds\n",
      "iteration 84 finished in 7.534956 seconds\n",
      "iteration 85 finished in 7.531712 seconds\n",
      "iteration 86 finished in 7.517253 seconds\n",
      "iteration 87 finished in 7.520298 seconds\n",
      "iteration 88 finished in 7.521376 seconds\n",
      "iteration 89 finished in 7.532296 seconds\n",
      "iteration 90 finished in 7.515125 seconds\n",
      "iteration 91 finished in 7.505958 seconds\n",
      "iteration 92 finished in 7.538306 seconds\n",
      "iteration 93 finished in 7.479462 seconds\n",
      "iteration 94 finished in 7.482511 seconds\n",
      "iteration 95 finished in 7.557141 seconds\n",
      "iteration 96 finished in 7.621312 seconds\n",
      "iteration 97 finished in 7.505341 seconds\n",
      "iteration 98 finished in 7.476631 seconds\n",
      "iteration 99 finished in 7.510115 seconds\n",
      "iteration 100 finished in 7.473245 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99a072095f043229d05f47f70426bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.3749542317770752\n"
     ]
    }
   ],
   "source": [
    "# get booking records for visitors in the validation set\n",
    "#user_with_booked_prop = final_val[final_val.prop_booking_bool == 1][['srch_id', 'srch_visitor_id', 'prop_key']]\n",
    "#booked_prop_groupby_user = user_with_booked_prop.groupby('srch_visitor_id')['prop_key'].apply(list).reset_index(name='booked_prop')\n",
    "\n",
    "result_ordered_2 = latent_factor_model_training(20, 100, 2)\n",
    "dcg_for_all_users_2 = latent_factor_model_evaluation(result_ordered_2, final_val, booked_prop_groupby_user)\n",
    "print(dcg_for_all_users_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent factor model 3: 60 latent facotrs, 150 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 finished in 8.547319 seconds\n",
      "iteration 2 finished in 8.036836 seconds\n",
      "iteration 3 finished in 8.074390 seconds\n",
      "iteration 4 finished in 8.061693 seconds\n",
      "iteration 5 finished in 8.017559 seconds\n",
      "iteration 6 finished in 8.080949 seconds\n",
      "iteration 7 finished in 8.053395 seconds\n",
      "iteration 8 finished in 7.966269 seconds\n",
      "iteration 9 finished in 8.040052 seconds\n",
      "iteration 10 finished in 8.048553 seconds\n",
      "iteration 11 finished in 8.385144 seconds\n",
      "iteration 12 finished in 8.039587 seconds\n",
      "iteration 13 finished in 7.982268 seconds\n",
      "iteration 14 finished in 7.983535 seconds\n",
      "iteration 15 finished in 8.242350 seconds\n",
      "iteration 16 finished in 8.086089 seconds\n",
      "iteration 17 finished in 8.008594 seconds\n",
      "iteration 18 finished in 7.997565 seconds\n",
      "iteration 19 finished in 8.055492 seconds\n",
      "iteration 20 finished in 8.029889 seconds\n",
      "iteration 21 finished in 8.008946 seconds\n",
      "iteration 22 finished in 8.180729 seconds\n",
      "iteration 23 finished in 8.071341 seconds\n",
      "iteration 24 finished in 7.989797 seconds\n",
      "iteration 25 finished in 8.053179 seconds\n",
      "iteration 26 finished in 7.979307 seconds\n",
      "iteration 27 finished in 8.033823 seconds\n",
      "iteration 28 finished in 8.054417 seconds\n",
      "iteration 29 finished in 7.699437 seconds\n",
      "iteration 30 finished in 7.675184 seconds\n",
      "iteration 31 finished in 7.658314 seconds\n",
      "iteration 32 finished in 8.014254 seconds\n",
      "iteration 33 finished in 8.262698 seconds\n",
      "iteration 34 finished in 8.190306 seconds\n",
      "iteration 35 finished in 8.015163 seconds\n",
      "iteration 36 finished in 7.990252 seconds\n",
      "iteration 37 finished in 7.943225 seconds\n",
      "iteration 38 finished in 7.965534 seconds\n",
      "iteration 39 finished in 7.949982 seconds\n",
      "iteration 40 finished in 8.005923 seconds\n",
      "iteration 41 finished in 8.879504 seconds\n",
      "iteration 42 finished in 8.336446 seconds\n",
      "iteration 43 finished in 9.335313 seconds\n",
      "iteration 44 finished in 8.424189 seconds\n",
      "iteration 45 finished in 7.737279 seconds\n",
      "iteration 46 finished in 7.770287 seconds\n",
      "iteration 47 finished in 8.018623 seconds\n",
      "iteration 48 finished in 7.659590 seconds\n",
      "iteration 49 finished in 7.679331 seconds\n",
      "iteration 50 finished in 7.601452 seconds\n",
      "iteration 51 finished in 7.744864 seconds\n",
      "iteration 52 finished in 7.644679 seconds\n",
      "iteration 53 finished in 8.216676 seconds\n",
      "iteration 54 finished in 7.817578 seconds\n",
      "iteration 55 finished in 7.836258 seconds\n",
      "iteration 56 finished in 7.824785 seconds\n",
      "iteration 57 finished in 7.910330 seconds\n",
      "iteration 58 finished in 8.559071 seconds\n",
      "iteration 59 finished in 8.651177 seconds\n",
      "iteration 60 finished in 8.318779 seconds\n",
      "iteration 61 finished in 7.861530 seconds\n",
      "iteration 62 finished in 7.834003 seconds\n",
      "iteration 63 finished in 7.985081 seconds\n",
      "iteration 64 finished in 8.024471 seconds\n",
      "iteration 65 finished in 8.038169 seconds\n",
      "iteration 66 finished in 8.009562 seconds\n",
      "iteration 67 finished in 7.860015 seconds\n",
      "iteration 68 finished in 7.846853 seconds\n",
      "iteration 69 finished in 7.781145 seconds\n",
      "iteration 70 finished in 8.415138 seconds\n",
      "iteration 71 finished in 7.934985 seconds\n",
      "iteration 72 finished in 7.835918 seconds\n",
      "iteration 73 finished in 7.899683 seconds\n",
      "iteration 74 finished in 7.838172 seconds\n",
      "iteration 75 finished in 7.834769 seconds\n",
      "iteration 76 finished in 7.826957 seconds\n",
      "iteration 77 finished in 7.946006 seconds\n",
      "iteration 78 finished in 7.852571 seconds\n",
      "iteration 79 finished in 7.818772 seconds\n",
      "iteration 80 finished in 7.836638 seconds\n",
      "iteration 81 finished in 7.868190 seconds\n",
      "iteration 82 finished in 7.857290 seconds\n",
      "iteration 83 finished in 7.853156 seconds\n",
      "iteration 84 finished in 7.827017 seconds\n",
      "iteration 85 finished in 7.915920 seconds\n",
      "iteration 86 finished in 7.900963 seconds\n",
      "iteration 87 finished in 7.899651 seconds\n",
      "iteration 88 finished in 7.834359 seconds\n",
      "iteration 89 finished in 7.836534 seconds\n",
      "iteration 90 finished in 7.888311 seconds\n",
      "iteration 91 finished in 7.868185 seconds\n",
      "iteration 92 finished in 7.877743 seconds\n",
      "iteration 93 finished in 7.885813 seconds\n",
      "iteration 94 finished in 7.868918 seconds\n",
      "iteration 95 finished in 7.861907 seconds\n",
      "iteration 96 finished in 7.851131 seconds\n",
      "iteration 97 finished in 7.898012 seconds\n",
      "iteration 98 finished in 8.173264 seconds\n",
      "iteration 99 finished in 8.379548 seconds\n",
      "iteration 100 finished in 8.108850 seconds\n",
      "iteration 101 finished in 7.971487 seconds\n",
      "iteration 102 finished in 8.302914 seconds\n",
      "iteration 103 finished in 8.308165 seconds\n",
      "iteration 104 finished in 8.376104 seconds\n",
      "iteration 105 finished in 8.231870 seconds\n",
      "iteration 106 finished in 8.099970 seconds\n",
      "iteration 107 finished in 7.942733 seconds\n",
      "iteration 108 finished in 7.933321 seconds\n",
      "iteration 109 finished in 7.917244 seconds\n",
      "iteration 110 finished in 7.909971 seconds\n",
      "iteration 111 finished in 7.913341 seconds\n",
      "iteration 112 finished in 7.888431 seconds\n",
      "iteration 113 finished in 7.901857 seconds\n",
      "iteration 114 finished in 7.991425 seconds\n",
      "iteration 115 finished in 8.011896 seconds\n",
      "iteration 116 finished in 8.014217 seconds\n",
      "iteration 117 finished in 8.041044 seconds\n",
      "iteration 118 finished in 7.781089 seconds\n",
      "iteration 119 finished in 7.783286 seconds\n",
      "iteration 120 finished in 7.850449 seconds\n",
      "iteration 121 finished in 7.827603 seconds\n",
      "iteration 122 finished in 7.850653 seconds\n",
      "iteration 123 finished in 7.819384 seconds\n",
      "iteration 124 finished in 7.819619 seconds\n",
      "iteration 125 finished in 7.951711 seconds\n",
      "iteration 126 finished in 8.130115 seconds\n",
      "iteration 127 finished in 8.101352 seconds\n",
      "iteration 128 finished in 8.442947 seconds\n",
      "iteration 129 finished in 8.927064 seconds\n",
      "iteration 130 finished in 7.930033 seconds\n",
      "iteration 131 finished in 7.893262 seconds\n",
      "iteration 132 finished in 7.866404 seconds\n",
      "iteration 133 finished in 7.856926 seconds\n",
      "iteration 134 finished in 8.229541 seconds\n",
      "iteration 135 finished in 7.917696 seconds\n",
      "iteration 136 finished in 8.161938 seconds\n",
      "iteration 137 finished in 7.791336 seconds\n",
      "iteration 138 finished in 7.874292 seconds\n",
      "iteration 139 finished in 7.823734 seconds\n",
      "iteration 140 finished in 7.804303 seconds\n",
      "iteration 141 finished in 7.809707 seconds\n",
      "iteration 142 finished in 7.796848 seconds\n",
      "iteration 143 finished in 7.749758 seconds\n",
      "iteration 144 finished in 7.840462 seconds\n",
      "iteration 145 finished in 7.817252 seconds\n",
      "iteration 146 finished in 7.826513 seconds\n",
      "iteration 147 finished in 8.740915 seconds\n",
      "iteration 148 finished in 7.778867 seconds\n",
      "iteration 149 finished in 7.703466 seconds\n",
      "iteration 150 finished in 7.651000 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e73b5d824b4f42b533c488e4a3aac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.45782058604718023\n"
     ]
    }
   ],
   "source": [
    "result_ordered_3 = latent_factor_model_training(60, 150, 3)\n",
    "dcg_for_all_users_3 = latent_factor_model_evaluation(result_ordered_3, final_val, booked_prop_groupby_user)\n",
    "print(dcg_for_all_users_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent factor model 4: 125 latent factors, 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 finished in 9.138504 seconds\n",
      "iteration 2 finished in 8.635266 seconds\n",
      "iteration 3 finished in 8.563531 seconds\n",
      "iteration 4 finished in 9.103434 seconds\n",
      "iteration 5 finished in 8.643828 seconds\n",
      "iteration 6 finished in 8.512614 seconds\n",
      "iteration 7 finished in 8.527029 seconds\n",
      "iteration 8 finished in 8.574532 seconds\n",
      "iteration 9 finished in 8.506694 seconds\n",
      "iteration 10 finished in 8.555544 seconds\n",
      "iteration 11 finished in 8.570706 seconds\n",
      "iteration 12 finished in 8.843381 seconds\n",
      "iteration 13 finished in 8.510568 seconds\n",
      "iteration 14 finished in 8.527646 seconds\n",
      "iteration 15 finished in 8.551603 seconds\n",
      "iteration 16 finished in 8.551183 seconds\n",
      "iteration 17 finished in 9.299714 seconds\n",
      "iteration 18 finished in 8.926976 seconds\n",
      "iteration 19 finished in 8.558219 seconds\n",
      "iteration 20 finished in 8.401079 seconds\n",
      "iteration 21 finished in 9.238536 seconds\n",
      "iteration 22 finished in 9.055114 seconds\n",
      "iteration 23 finished in 8.576230 seconds\n",
      "iteration 24 finished in 8.589538 seconds\n",
      "iteration 25 finished in 8.725160 seconds\n",
      "iteration 26 finished in 8.485686 seconds\n",
      "iteration 27 finished in 8.373279 seconds\n",
      "iteration 28 finished in 8.564342 seconds\n",
      "iteration 29 finished in 9.129259 seconds\n",
      "iteration 30 finished in 8.245827 seconds\n",
      "iteration 31 finished in 8.565800 seconds\n",
      "iteration 32 finished in 8.505440 seconds\n",
      "iteration 33 finished in 8.851361 seconds\n",
      "iteration 34 finished in 8.913794 seconds\n",
      "iteration 35 finished in 8.953893 seconds\n",
      "iteration 36 finished in 9.025466 seconds\n",
      "iteration 37 finished in 8.300299 seconds\n",
      "iteration 38 finished in 8.267410 seconds\n",
      "iteration 39 finished in 8.292616 seconds\n",
      "iteration 40 finished in 8.301451 seconds\n",
      "iteration 41 finished in 8.440659 seconds\n",
      "iteration 42 finished in 8.371230 seconds\n",
      "iteration 43 finished in 8.544775 seconds\n",
      "iteration 44 finished in 8.423787 seconds\n",
      "iteration 45 finished in 8.210990 seconds\n",
      "iteration 46 finished in 8.207340 seconds\n",
      "iteration 47 finished in 8.237527 seconds\n",
      "iteration 48 finished in 8.189521 seconds\n",
      "iteration 49 finished in 8.226220 seconds\n",
      "iteration 50 finished in 8.178549 seconds\n",
      "iteration 51 finished in 8.213122 seconds\n",
      "iteration 52 finished in 8.161775 seconds\n",
      "iteration 53 finished in 8.182562 seconds\n",
      "iteration 54 finished in 8.354132 seconds\n",
      "iteration 55 finished in 8.421762 seconds\n",
      "iteration 56 finished in 8.312728 seconds\n",
      "iteration 57 finished in 8.221666 seconds\n",
      "iteration 58 finished in 8.287026 seconds\n",
      "iteration 59 finished in 9.249165 seconds\n",
      "iteration 60 finished in 8.949328 seconds\n",
      "iteration 61 finished in 8.923532 seconds\n",
      "iteration 62 finished in 8.775129 seconds\n",
      "iteration 63 finished in 8.928024 seconds\n",
      "iteration 64 finished in 8.362276 seconds\n",
      "iteration 65 finished in 8.458491 seconds\n",
      "iteration 66 finished in 8.727509 seconds\n",
      "iteration 67 finished in 8.720067 seconds\n",
      "iteration 68 finished in 8.438142 seconds\n",
      "iteration 69 finished in 8.492288 seconds\n",
      "iteration 70 finished in 8.501659 seconds\n",
      "iteration 71 finished in 8.575700 seconds\n",
      "iteration 72 finished in 8.539860 seconds\n",
      "iteration 73 finished in 8.490713 seconds\n",
      "iteration 74 finished in 8.528282 seconds\n",
      "iteration 75 finished in 8.483565 seconds\n",
      "iteration 76 finished in 8.517379 seconds\n",
      "iteration 77 finished in 8.573402 seconds\n",
      "iteration 78 finished in 8.515375 seconds\n",
      "iteration 79 finished in 8.492313 seconds\n",
      "iteration 80 finished in 8.465067 seconds\n",
      "iteration 81 finished in 8.524479 seconds\n",
      "iteration 82 finished in 8.414829 seconds\n",
      "iteration 83 finished in 8.551682 seconds\n",
      "iteration 84 finished in 8.556720 seconds\n",
      "iteration 85 finished in 8.461617 seconds\n",
      "iteration 86 finished in 8.505647 seconds\n",
      "iteration 87 finished in 8.445197 seconds\n",
      "iteration 88 finished in 8.453619 seconds\n",
      "iteration 89 finished in 8.449763 seconds\n",
      "iteration 90 finished in 8.472588 seconds\n",
      "iteration 91 finished in 8.507950 seconds\n",
      "iteration 92 finished in 8.465728 seconds\n",
      "iteration 93 finished in 8.467900 seconds\n",
      "iteration 94 finished in 8.458415 seconds\n",
      "iteration 95 finished in 8.430715 seconds\n",
      "iteration 96 finished in 8.448199 seconds\n",
      "iteration 97 finished in 8.551094 seconds\n",
      "iteration 98 finished in 8.501454 seconds\n",
      "iteration 99 finished in 8.898503 seconds\n",
      "iteration 100 finished in 8.290966 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fc941814ae483e831c29b78869c99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.4611140349019519\n"
     ]
    }
   ],
   "source": [
    "result_ordered_4 = latent_factor_model_training(125, 100, 4)\n",
    "dcg_for_all_users_4 = latent_factor_model_evaluation(result_ordered_4, final_val, booked_prop_groupby_user)\n",
    "print(dcg_for_all_users_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "The latent factor model with 125 latent factors and 100 iterations have the highest DCG@10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model\n",
    "### Incorporating popularity, price, and average ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensemble_model(train_set, latent_factor_res, latent_weight, popu_weight, price_weight, review_weight):\n",
    "    \n",
    "    def create_popu_price_review_df(train_set):\n",
    "        # get the number of bookings, average price, and average ratings for properties in the train set        \n",
    "        train_groupby_columns = train_set.groupby(\"prop_key\").agg(\n",
    "                {'prop_booking_bool': np.sum, \n",
    "                 'prop_price_with_discount_usd' : lambda x : np.mean(x),\n",
    "                 'prop_review_score': lambda x :  np.mean(x)})\n",
    "    \n",
    "        return train_groupby_columns\n",
    "    \n",
    "    def scale_to_zero_one_range(grouped_df):\n",
    "        # normalize the data to 0 to 1\n",
    "        scaler = MinMaxScaler()\n",
    "        grouped_df_scaled = scaler.fit_transform(grouped_df)\n",
    "        scaled_df = pd.DataFrame(grouped_df_scaled, \n",
    "                                 columns = grouped_df.columns,\n",
    "                                index = grouped_df.index)\n",
    "        scaled_df.reset_index(inplace=True)\n",
    "        \n",
    "        return scaled_df\n",
    "    \n",
    "    def scale_latent_score(latent_factor_res):\n",
    "        scaler = MinMaxScaler()\n",
    "        latent_factor_res['scaled_latent_factor_score'] = scaler.fit_transform(latent_factor_res[['score']])\n",
    "        latent_factor_remove_orig_score = latent_factor_res.drop(['score'], axis=1)\n",
    "        \n",
    "        return latent_factor_remove_orig_score    \n",
    "    \n",
    "    def combine_score(latent_factor_result, popu_price_review_result):\n",
    "        combined_df = latent_factor_result.join(popu_price_review_result.set_index('prop_key'), on = 'prop_key', how = 'inner')\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    def linear_combine_score(latent_weight, popu_weight, price_weight, review_weight, df):\n",
    "        df['score'] = latent_weight * df['scaled_latent_factor_score'] + \\\n",
    "                                 popu_weight * df['prop_booking_bool'] + \\\n",
    "                                 price_weight * df['prop_price_with_discount_usd'] + \\\n",
    "                                 review_weight * df['prop_review_score']\n",
    "                    \n",
    "        return df\n",
    "    \n",
    "    def reorder_and_remove_other_columns(df):\n",
    "        results_ordered = df.sort_values(by=['srch_id', 'score'], ascending = [True, False])\n",
    "        final_result = results_ordered[['srch_id', 'prop_key']]\n",
    "        \n",
    "        return final_result\n",
    "        \n",
    "    popu_price_review_df = create_popu_price_review_df(train_set)\n",
    "    popu_price_review_df_scaled = scale_to_zero_one_range(popu_price_review_df)\n",
    "    latent_score_scaled = scale_latent_score(latent_factor_res)\n",
    "    \n",
    "    combined_df = combine_score(latent_score_scaled, popu_price_review_df_scaled)\n",
    "    df_final_score = linear_combine_score(latent_weight, popu_weight, price_weight, review_weight, combined_df)\n",
    "    result_final_combined_score = reorder_and_remove_other_columns(df_final_score)\n",
    "\n",
    "    return result_final_combined_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemble_sample_res = ensemble_model(final_train, result_ordered_4, 1, 5, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43863247870470323\n"
     ]
    }
   ],
   "source": [
    "dcg_for_all_users = latent_factor_model_evaluation(ensemble_sample_res, final_val, booked_prop_groupby_user)\n",
    "print(dcg_for_all_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights tuning based on grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_weight_grid = [0.03, 0.3, 3, 30]\n",
    "popu_weight_grid = [0.03, 0.3, 3, 30]\n",
    "price_weight_grid = [-0.03, -0.3, -3, -30]\n",
    "review_weight_grid = [0.03, 0.3, 3, 30]\n",
    "grid_search_columns = [\"latent_weight\", \"popu_weight\", \"price_weight\", \"review_weight\", \"dcg_score\"]\n",
    "data = []\n",
    "\n",
    "for la in latent_weight_grid:\n",
    "    for po in popu_weight_grid:\n",
    "        for pr in price_weight_grid:\n",
    "            for re in review_weight_grid:\n",
    "                params_value = {\"train_set\": final_train,\n",
    "                                \"latent_factor_res\": result_ordered_4,\n",
    "                                \"latent_weight\": la, \n",
    "                                \"popu_weight\": po, \n",
    "                                \"price_weight\": pr, \n",
    "                                \"review_weight\": re}\n",
    "                \n",
    "                ensemble_res = ensemble_model(**params_value)\n",
    "                dcg_for_all_users = latent_factor_model_evaluation(ensemble_res, final_val, booked_prop_groupby_user)\n",
    "                data.append([la, po, pr, re, dcg_for_all_users])\n",
    "                \n",
    "grid_search_result = pd.DataFrame(data, columns=grid_search_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latent_weight</th>\n",
       "      <th>popu_weight</th>\n",
       "      <th>price_weight</th>\n",
       "      <th>review_weight</th>\n",
       "      <th>dcg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.487499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.487499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.485009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.481040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.476011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.476011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.476011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.475627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.475627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.475627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.475339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.474466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.474324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.474324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.474295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.474295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.473918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.473918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.473918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.473526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.473526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.473526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.472328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.472328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.472328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.467632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.467473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.467473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.467473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.467473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.467473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.465914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.465760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.465760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.465760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.465664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.465249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.465249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.465249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.465244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.463774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.463774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.463696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.463696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.463276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.463276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.462427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.462427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.462427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.454749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.227623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.227623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.216077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.216077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.214322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.214235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.214235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.214235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.214110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.206495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.206495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.206495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.204653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.204653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.203428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.30</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.201139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.201139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.201032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.197982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.196462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.195998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.194542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.194542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.194494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.194494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.192651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.192651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.190433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.190433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.186498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.186415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.184144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.182985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.182985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.182176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.178669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.177474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.176827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.175192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.174979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.174555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.173349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.173349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.173099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.173099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.170953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.170953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.169163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.163690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     latent_weight  popu_weight  price_weight  review_weight  dcg_score\n",
       "72            0.30         0.03         -3.00           0.03   0.487499\n",
       "157           3.00         0.30        -30.00           0.30   0.487499\n",
       "140           3.00         0.03        -30.00           0.03   0.485009\n",
       "141           3.00         0.03        -30.00           0.30   0.481040\n",
       "68            0.30         0.03         -0.30           0.03   0.476011\n",
       "238          30.00         3.00        -30.00           3.00   0.476011\n",
       "153           3.00         0.30         -3.00           0.30   0.476011\n",
       "232          30.00         3.00         -3.00           0.03   0.475627\n",
       "224          30.00         3.00         -0.03           0.03   0.475627\n",
       "228          30.00         3.00         -0.30           0.03   0.475627\n",
       "156           3.00         0.30        -30.00           0.03   0.475339\n",
       "236          30.00         3.00        -30.00           0.03   0.474466\n",
       "237          30.00         3.00        -30.00           0.30   0.474324\n",
       "152           3.00         0.30         -3.00           0.03   0.474324\n",
       "229          30.00         3.00         -0.30           0.30   0.474295\n",
       "144           3.00         0.30         -0.03           0.03   0.474295\n",
       "145           3.00         0.30         -0.03           0.30   0.473918\n",
       "230          30.00         3.00         -0.30           3.00   0.473918\n",
       "226          30.00         3.00         -0.03           3.00   0.473918\n",
       "149           3.00         0.30         -0.30           0.30   0.473526\n",
       "64            0.30         0.03         -0.03           0.03   0.473526\n",
       "234          30.00         3.00         -3.00           3.00   0.473526\n",
       "148           3.00         0.30         -0.30           0.03   0.472328\n",
       "225          30.00         3.00         -0.03           0.30   0.472328\n",
       "233          30.00         3.00         -3.00           0.30   0.472328\n",
       "204          30.00         0.03        -30.00           0.03   0.467632\n",
       "209          30.00         0.30         -0.03           0.30   0.467473\n",
       "132           3.00         0.03         -0.30           0.03   0.467473\n",
       "128           3.00         0.03         -0.03           0.03   0.467473\n",
       "217          30.00         0.30         -3.00           0.30   0.467473\n",
       "213          30.00         0.30         -0.30           0.30   0.467473\n",
       "220          30.00         0.30        -30.00           0.03   0.465914\n",
       "197          30.00         0.03         -0.30           0.30   0.465760\n",
       "201          30.00         0.03         -3.00           0.30   0.465760\n",
       "193          30.00         0.03         -0.03           0.30   0.465760\n",
       "205          30.00         0.03        -30.00           0.30   0.465664\n",
       "196          30.00         0.03         -0.30           0.03   0.465249\n",
       "200          30.00         0.03         -3.00           0.03   0.465249\n",
       "192          30.00         0.03         -0.03           0.03   0.465249\n",
       "208          30.00         0.30         -0.03           0.03   0.465244\n",
       "137           3.00         0.03         -3.00           0.30   0.463774\n",
       "222          30.00         0.30        -30.00           3.00   0.463774\n",
       "221          30.00         0.30        -30.00           0.30   0.463696\n",
       "136           3.00         0.03         -3.00           0.03   0.463696\n",
       "212          30.00         0.30         -0.30           0.03   0.463276\n",
       "216          30.00         0.30         -3.00           0.03   0.463276\n",
       "89            0.30         0.30         -3.00           0.30   0.462427\n",
       "4             0.03         0.03         -0.30           0.03   0.462427\n",
       "174           3.00         3.00        -30.00           3.00   0.462427\n",
       "210          30.00         0.30         -0.03           3.00   0.454749\n",
       "..             ...          ...           ...            ...        ...\n",
       "175           3.00         3.00        -30.00          30.00   0.227623\n",
       "5             0.03         0.03         -0.30           0.30   0.227623\n",
       "82            0.30         0.30         -0.03           3.00   0.216077\n",
       "167           3.00         3.00         -0.30          30.00   0.216077\n",
       "14            0.03         0.03        -30.00           3.00   0.214322\n",
       "1             0.03         0.03         -0.03           0.30   0.214235\n",
       "171           3.00         3.00         -3.00          30.00   0.214235\n",
       "86            0.30         0.30         -0.30           3.00   0.214235\n",
       "163           3.00         3.00         -0.03          30.00   0.214110\n",
       "99            0.30         3.00         -0.03          30.00   0.206495\n",
       "103           0.30         3.00         -0.30          30.00   0.206495\n",
       "18            0.03         0.30         -0.03           3.00   0.206495\n",
       "22            0.03         0.30         -0.30           3.00   0.204653\n",
       "107           0.30         3.00         -3.00          30.00   0.204653\n",
       "43            0.03         3.00         -3.00          30.00   0.203428\n",
       "39            0.03         3.00         -0.30          30.00   0.203000\n",
       "111           0.30         3.00        -30.00          30.00   0.201139\n",
       "26            0.03         0.30         -3.00           3.00   0.201139\n",
       "35            0.03         3.00         -0.03          30.00   0.201032\n",
       "47            0.03         3.00        -30.00          30.00   0.197982\n",
       "147           3.00         0.30         -0.03          30.00   0.196462\n",
       "143           3.00         0.03        -30.00          30.00   0.195998\n",
       "159           3.00         0.30        -30.00          30.00   0.194542\n",
       "74            0.30         0.03         -3.00           3.00   0.194542\n",
       "66            0.30         0.03         -0.03           3.00   0.194494\n",
       "151           3.00         0.30         -0.30          30.00   0.194494\n",
       "70            0.30         0.03         -0.30           3.00   0.192651\n",
       "155           3.00         0.30         -3.00          30.00   0.192651\n",
       "131           3.00         0.03         -0.03          30.00   0.190433\n",
       "135           3.00         0.03         -0.30          30.00   0.190433\n",
       "139           3.00         0.03         -3.00          30.00   0.186498\n",
       "27            0.03         0.30         -3.00          30.00   0.186415\n",
       "23            0.03         0.30         -0.30          30.00   0.184144\n",
       "10            0.03         0.03         -3.00           3.00   0.182985\n",
       "95            0.30         0.30        -30.00          30.00   0.182985\n",
       "19            0.03         0.30         -0.03          30.00   0.182176\n",
       "11            0.03         0.03         -3.00          30.00   0.178669\n",
       "31            0.03         0.30        -30.00          30.00   0.177474\n",
       "7             0.03         0.03         -0.30          30.00   0.176827\n",
       "83            0.30         0.30         -0.03          30.00   0.175192\n",
       "79            0.30         0.03        -30.00          30.00   0.174979\n",
       "3             0.03         0.03         -0.03          30.00   0.174555\n",
       "87            0.30         0.30         -0.30          30.00   0.173349\n",
       "2             0.03         0.03         -0.03           3.00   0.173349\n",
       "91            0.30         0.30         -3.00          30.00   0.173099\n",
       "6             0.03         0.03         -0.30           3.00   0.173099\n",
       "71            0.30         0.03         -0.30          30.00   0.170953\n",
       "67            0.30         0.03         -0.03          30.00   0.170953\n",
       "75            0.30         0.03         -3.00          30.00   0.169163\n",
       "15            0.03         0.03        -30.00          30.00   0.163690\n",
       "\n",
       "[256 rows x 5 columns]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_result.sort_values(['dcg_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_weight_grid = [0.5, 1, 1.5]\n",
    "popu_weight_grid = [0.05, 0.1, 0.2]\n",
    "price_weight_grid = [-10, -15, -25]\n",
    "review_weight_grid = [0.05, 0.1, 0.2]\n",
    "grid_search_columns = [\"latent_weight\", \"popu_weight\", \"price_weight\", \"review_weight\", \"dcg_score\"]\n",
    "data_2 = []\n",
    "\n",
    "for la in latent_weight_grid:\n",
    "    for po in popu_weight_grid:\n",
    "        for pr in price_weight_grid:\n",
    "            for re in review_weight_grid:\n",
    "                params_value = {\"train_set\": final_train,\n",
    "                                \"latent_factor_res\": result_ordered_4,\n",
    "                                \"latent_weight\": la, \n",
    "                                \"popu_weight\": po, \n",
    "                                \"price_weight\": pr, \n",
    "                                \"review_weight\": re}\n",
    "                \n",
    "                ensemble_res = ensemble_model(**params_value)\n",
    "                dcg_for_all_users = latent_factor_model_evaluation(ensemble_res, final_val, booked_prop_groupby_user)\n",
    "                data_2.append([la, po, pr, re, dcg_for_all_users])\n",
    "                \n",
    "grid_search_result_2 = pd.DataFrame(data_2, columns=grid_search_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latent_weight</th>\n",
       "      <th>popu_weight</th>\n",
       "      <th>price_weight</th>\n",
       "      <th>review_weight</th>\n",
       "      <th>dcg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.495463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.490734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.487566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.487499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.487224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.487065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.486973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.484445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.482357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.481937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.481883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.481021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.480790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.479623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.479134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.478729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.478262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.477616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.477503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.476257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.473634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.472224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.472088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.471752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.471746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.469317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.468538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.468064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.467219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.465966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.464361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.461668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.460259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.458922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.456289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.455782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.455538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.455389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.455253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.453213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.453189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.452454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.451730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.451405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.449309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.448703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.448312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.448293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.447171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.447042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.446735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.445722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.444155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.443841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.441093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.440810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.440691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.440667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.437010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.436498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.434275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.432972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.432507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.432191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.430961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.430261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.430095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.426103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.425725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.425369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.424719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.423148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.422748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.419656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.419367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.416711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.416619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.415066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.409441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.409292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.408955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latent_weight  popu_weight  price_weight  review_weight  dcg_score\n",
       "65            1.5         0.10           -10           0.20   0.495463\n",
       "55            1.5         0.05           -10           0.10   0.490734\n",
       "64            1.5         0.10           -10           0.10   0.487566\n",
       "37            1.0         0.10           -10           0.10   0.487499\n",
       "73            1.5         0.20           -10           0.10   0.487224\n",
       "63            1.5         0.10           -10           0.05   0.487065\n",
       "72            1.5         0.20           -10           0.05   0.486973\n",
       "75            1.5         0.20           -15           0.05   0.484445\n",
       "36            1.0         0.10           -10           0.05   0.482357\n",
       "67            1.5         0.10           -15           0.10   0.481937\n",
       "59            1.5         0.05           -15           0.20   0.481883\n",
       "54            1.5         0.05           -10           0.05   0.481021\n",
       "58            1.5         0.05           -15           0.10   0.480790\n",
       "76            1.5         0.20           -15           0.10   0.479623\n",
       "68            1.5         0.10           -15           0.20   0.479134\n",
       "29            1.0         0.05           -10           0.20   0.478729\n",
       "66            1.5         0.10           -15           0.05   0.478262\n",
       "28            1.0         0.05           -10           0.10   0.477616\n",
       "74            1.5         0.20           -10           0.20   0.477503\n",
       "56            1.5         0.05           -10           0.20   0.476257\n",
       "40            1.0         0.10           -15           0.10   0.473634\n",
       "27            1.0         0.05           -10           0.05   0.472224\n",
       "57            1.5         0.05           -15           0.05   0.472088\n",
       "45            1.0         0.20           -10           0.05   0.471752\n",
       "46            1.0         0.20           -10           0.10   0.471746\n",
       "39            1.0         0.10           -15           0.05   0.469317\n",
       "77            1.5         0.20           -15           0.20   0.468538\n",
       "38            1.0         0.10           -10           0.20   0.468064\n",
       "48            1.0         0.20           -15           0.05   0.467219\n",
       "47            1.0         0.20           -10           0.20   0.465966\n",
       "41            1.0         0.10           -15           0.20   0.464361\n",
       "71            1.5         0.10           -25           0.20   0.461668\n",
       "31            1.0         0.05           -15           0.10   0.460259\n",
       "30            1.0         0.05           -15           0.05   0.458922\n",
       "49            1.0         0.20           -15           0.10   0.456289\n",
       "78            1.5         0.20           -25           0.05   0.455782\n",
       "79            1.5         0.20           -25           0.10   0.455538\n",
       "70            1.5         0.10           -25           0.10   0.455389\n",
       "69            1.5         0.10           -25           0.05   0.455253\n",
       "32            1.0         0.05           -15           0.20   0.453213\n",
       "80            1.5         0.20           -25           0.20   0.453189\n",
       "16            0.5         0.10           -25           0.10   0.452454\n",
       "50            1.0         0.20           -15           0.20   0.451730\n",
       "10            0.5         0.10           -10           0.10   0.451405\n",
       "1             0.5         0.05           -10           0.10   0.449309\n",
       "0             0.5         0.05           -10           0.05   0.448703\n",
       "9             0.5         0.10           -10           0.05   0.448312\n",
       "43            1.0         0.10           -25           0.10   0.448293\n",
       "15            0.5         0.10           -25           0.05   0.447171\n",
       "44            1.0         0.10           -25           0.20   0.447042\n",
       "60            1.5         0.05           -25           0.05   0.446735\n",
       "42            1.0         0.10           -25           0.05   0.445722\n",
       "18            0.5         0.20           -10           0.05   0.444155\n",
       "6             0.5         0.05           -25           0.05   0.443841\n",
       "62            1.5         0.05           -25           0.20   0.441093\n",
       "19            0.5         0.20           -10           0.10   0.440810\n",
       "53            1.0         0.20           -25           0.20   0.440691\n",
       "52            1.0         0.20           -25           0.10   0.440667\n",
       "20            0.5         0.20           -10           0.20   0.437010\n",
       "13            0.5         0.10           -15           0.10   0.436498\n",
       "4             0.5         0.05           -15           0.10   0.434275\n",
       "61            1.5         0.05           -25           0.10   0.432972\n",
       "12            0.5         0.10           -15           0.05   0.432507\n",
       "11            0.5         0.10           -10           0.20   0.432191\n",
       "14            0.5         0.10           -15           0.20   0.430961\n",
       "3             0.5         0.05           -15           0.05   0.430261\n",
       "51            1.0         0.20           -25           0.05   0.430095\n",
       "34            1.0         0.05           -25           0.10   0.426103\n",
       "23            0.5         0.20           -15           0.20   0.425725\n",
       "33            1.0         0.05           -25           0.05   0.425369\n",
       "26            0.5         0.20           -25           0.20   0.424719\n",
       "17            0.5         0.10           -25           0.20   0.423148\n",
       "35            1.0         0.05           -25           0.20   0.422748\n",
       "7             0.5         0.05           -25           0.10   0.419656\n",
       "2             0.5         0.05           -10           0.20   0.419367\n",
       "24            0.5         0.20           -25           0.05   0.416711\n",
       "8             0.5         0.05           -25           0.20   0.416619\n",
       "21            0.5         0.20           -15           0.05   0.415066\n",
       "5             0.5         0.05           -15           0.20   0.409441\n",
       "22            0.5         0.20           -15           0.10   0.409292\n",
       "25            0.5         0.20           -25           0.10   0.408955"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_result_2.sort_values(['dcg_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_weight_grid = [1, 1.2, 1.4]\n",
    "popu_weight_grid = [0.06, 0.75, 0.09]\n",
    "price_weight_grid = [-10]\n",
    "review_weight_grid = [0.12, 0.15, 0.18]\n",
    "grid_search_columns = [\"latent_weight\", \"popu_weight\", \"price_weight\", \"review_weight\", \"dcg_score\"]\n",
    "data_3 = []\n",
    "\n",
    "for la in latent_weight_grid:\n",
    "    for po in popu_weight_grid:\n",
    "        for pr in price_weight_grid:\n",
    "            for re in review_weight_grid:\n",
    "                params_value = {\"train_set\": final_train,\n",
    "                                \"latent_factor_res\": result_ordered_4,\n",
    "                                \"latent_weight\": la, \n",
    "                                \"popu_weight\": po, \n",
    "                                \"price_weight\": pr, \n",
    "                                \"review_weight\": re}\n",
    "                \n",
    "                ensemble_res = ensemble_model(**params_value)\n",
    "                dcg_for_all_users = latent_factor_model_evaluation(ensemble_res, final_val, booked_prop_groupby_user)\n",
    "                data_3.append([la, po, pr, re, dcg_for_all_users])\n",
    "                \n",
    "grid_search_result_3 = pd.DataFrame(data_2, columns=grid_search_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latent_weight</th>\n",
       "      <th>popu_weight</th>\n",
       "      <th>price_weight</th>\n",
       "      <th>review_weight</th>\n",
       "      <th>dcg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.495463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.490734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.487566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.487499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.487224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.487065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.486973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.484445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.482357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.481937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.481883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.481021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.480790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.479623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.479134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.478729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.478262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.477616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.477503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.476257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.473634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.472224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.472088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.471752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.471746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.469317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.468538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.468064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.467219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.465966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.464361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.461668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.460259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.458922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.456289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.455782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.455538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.455389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.455253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.453213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.453189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.452454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.451730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.451405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.449309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.448703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.448312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.448293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.447171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.447042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.446735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.445722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.444155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.443841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.441093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.440810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.440691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.440667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.437010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.436498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.434275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.432972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.432507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.432191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.430961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.430261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.430095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.426103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.425725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.425369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.424719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.423148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.422748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.419656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.419367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.416711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.416619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.415066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.409441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.409292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.408955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latent_weight  popu_weight  price_weight  review_weight  dcg_score\n",
       "65            1.5         0.10           -10           0.20   0.495463\n",
       "55            1.5         0.05           -10           0.10   0.490734\n",
       "64            1.5         0.10           -10           0.10   0.487566\n",
       "37            1.0         0.10           -10           0.10   0.487499\n",
       "73            1.5         0.20           -10           0.10   0.487224\n",
       "63            1.5         0.10           -10           0.05   0.487065\n",
       "72            1.5         0.20           -10           0.05   0.486973\n",
       "75            1.5         0.20           -15           0.05   0.484445\n",
       "36            1.0         0.10           -10           0.05   0.482357\n",
       "67            1.5         0.10           -15           0.10   0.481937\n",
       "59            1.5         0.05           -15           0.20   0.481883\n",
       "54            1.5         0.05           -10           0.05   0.481021\n",
       "58            1.5         0.05           -15           0.10   0.480790\n",
       "76            1.5         0.20           -15           0.10   0.479623\n",
       "68            1.5         0.10           -15           0.20   0.479134\n",
       "29            1.0         0.05           -10           0.20   0.478729\n",
       "66            1.5         0.10           -15           0.05   0.478262\n",
       "28            1.0         0.05           -10           0.10   0.477616\n",
       "74            1.5         0.20           -10           0.20   0.477503\n",
       "56            1.5         0.05           -10           0.20   0.476257\n",
       "40            1.0         0.10           -15           0.10   0.473634\n",
       "27            1.0         0.05           -10           0.05   0.472224\n",
       "57            1.5         0.05           -15           0.05   0.472088\n",
       "45            1.0         0.20           -10           0.05   0.471752\n",
       "46            1.0         0.20           -10           0.10   0.471746\n",
       "39            1.0         0.10           -15           0.05   0.469317\n",
       "77            1.5         0.20           -15           0.20   0.468538\n",
       "38            1.0         0.10           -10           0.20   0.468064\n",
       "48            1.0         0.20           -15           0.05   0.467219\n",
       "47            1.0         0.20           -10           0.20   0.465966\n",
       "41            1.0         0.10           -15           0.20   0.464361\n",
       "71            1.5         0.10           -25           0.20   0.461668\n",
       "31            1.0         0.05           -15           0.10   0.460259\n",
       "30            1.0         0.05           -15           0.05   0.458922\n",
       "49            1.0         0.20           -15           0.10   0.456289\n",
       "78            1.5         0.20           -25           0.05   0.455782\n",
       "79            1.5         0.20           -25           0.10   0.455538\n",
       "70            1.5         0.10           -25           0.10   0.455389\n",
       "69            1.5         0.10           -25           0.05   0.455253\n",
       "32            1.0         0.05           -15           0.20   0.453213\n",
       "80            1.5         0.20           -25           0.20   0.453189\n",
       "16            0.5         0.10           -25           0.10   0.452454\n",
       "50            1.0         0.20           -15           0.20   0.451730\n",
       "10            0.5         0.10           -10           0.10   0.451405\n",
       "1             0.5         0.05           -10           0.10   0.449309\n",
       "0             0.5         0.05           -10           0.05   0.448703\n",
       "9             0.5         0.10           -10           0.05   0.448312\n",
       "43            1.0         0.10           -25           0.10   0.448293\n",
       "15            0.5         0.10           -25           0.05   0.447171\n",
       "44            1.0         0.10           -25           0.20   0.447042\n",
       "60            1.5         0.05           -25           0.05   0.446735\n",
       "42            1.0         0.10           -25           0.05   0.445722\n",
       "18            0.5         0.20           -10           0.05   0.444155\n",
       "6             0.5         0.05           -25           0.05   0.443841\n",
       "62            1.5         0.05           -25           0.20   0.441093\n",
       "19            0.5         0.20           -10           0.10   0.440810\n",
       "53            1.0         0.20           -25           0.20   0.440691\n",
       "52            1.0         0.20           -25           0.10   0.440667\n",
       "20            0.5         0.20           -10           0.20   0.437010\n",
       "13            0.5         0.10           -15           0.10   0.436498\n",
       "4             0.5         0.05           -15           0.10   0.434275\n",
       "61            1.5         0.05           -25           0.10   0.432972\n",
       "12            0.5         0.10           -15           0.05   0.432507\n",
       "11            0.5         0.10           -10           0.20   0.432191\n",
       "14            0.5         0.10           -15           0.20   0.430961\n",
       "3             0.5         0.05           -15           0.05   0.430261\n",
       "51            1.0         0.20           -25           0.05   0.430095\n",
       "34            1.0         0.05           -25           0.10   0.426103\n",
       "23            0.5         0.20           -15           0.20   0.425725\n",
       "33            1.0         0.05           -25           0.05   0.425369\n",
       "26            0.5         0.20           -25           0.20   0.424719\n",
       "17            0.5         0.10           -25           0.20   0.423148\n",
       "35            1.0         0.05           -25           0.20   0.422748\n",
       "7             0.5         0.05           -25           0.10   0.419656\n",
       "2             0.5         0.05           -10           0.20   0.419367\n",
       "24            0.5         0.20           -25           0.05   0.416711\n",
       "8             0.5         0.05           -25           0.20   0.416619\n",
       "21            0.5         0.20           -15           0.05   0.415066\n",
       "5             0.5         0.05           -15           0.20   0.409441\n",
       "22            0.5         0.20           -15           0.10   0.409292\n",
       "25            0.5         0.20           -25           0.10   0.408955"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_result_3.sort_values(['dcg_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final parameters\n",
    "- latent_weight = 1.5\n",
    "- popularity_weight = 0.1\n",
    "- price_weight = -10\n",
    "- review_weight = 0.2\n",
    "- number of latent factors: 125\n",
    "- training iterations: 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate results for the entire training set based on selected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_idx_mapping, users_idx_mapping, user_property_booking_matrix = create_user_property_matrix(train_processed, all_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 finished in 10.773543 seconds\n",
      "iteration 2 finished in 9.647498 seconds\n",
      "iteration 3 finished in 9.905503 seconds\n",
      "iteration 4 finished in 9.319237 seconds\n",
      "iteration 5 finished in 9.497583 seconds\n",
      "iteration 6 finished in 9.272575 seconds\n",
      "iteration 7 finished in 9.255271 seconds\n",
      "iteration 8 finished in 9.326726 seconds\n",
      "iteration 9 finished in 9.471587 seconds\n",
      "iteration 10 finished in 9.443905 seconds\n",
      "iteration 11 finished in 9.353863 seconds\n",
      "iteration 12 finished in 9.578443 seconds\n",
      "iteration 13 finished in 9.608833 seconds\n",
      "iteration 14 finished in 9.384641 seconds\n",
      "iteration 15 finished in 9.380008 seconds\n",
      "iteration 16 finished in 9.834795 seconds\n",
      "iteration 17 finished in 9.494722 seconds\n",
      "iteration 18 finished in 9.509276 seconds\n",
      "iteration 19 finished in 9.590548 seconds\n",
      "iteration 20 finished in 9.522426 seconds\n",
      "iteration 21 finished in 10.913183 seconds\n",
      "iteration 22 finished in 10.766535 seconds\n",
      "iteration 23 finished in 9.502840 seconds\n",
      "iteration 24 finished in 9.558577 seconds\n",
      "iteration 25 finished in 9.517244 seconds\n",
      "iteration 26 finished in 9.467613 seconds\n",
      "iteration 27 finished in 9.529071 seconds\n",
      "iteration 28 finished in 9.606040 seconds\n",
      "iteration 29 finished in 9.522339 seconds\n",
      "iteration 30 finished in 9.522681 seconds\n",
      "iteration 31 finished in 9.553430 seconds\n",
      "iteration 32 finished in 9.495628 seconds\n",
      "iteration 33 finished in 9.469049 seconds\n",
      "iteration 34 finished in 9.669087 seconds\n",
      "iteration 35 finished in 9.490527 seconds\n",
      "iteration 36 finished in 10.886944 seconds\n",
      "iteration 37 finished in 10.162800 seconds\n",
      "iteration 38 finished in 10.339229 seconds\n",
      "iteration 39 finished in 9.675882 seconds\n",
      "iteration 40 finished in 9.852846 seconds\n",
      "iteration 41 finished in 10.088273 seconds\n",
      "iteration 42 finished in 10.374661 seconds\n",
      "iteration 43 finished in 10.120564 seconds\n",
      "iteration 44 finished in 9.936236 seconds\n",
      "iteration 45 finished in 9.876865 seconds\n",
      "iteration 46 finished in 10.149317 seconds\n",
      "iteration 47 finished in 9.777125 seconds\n",
      "iteration 48 finished in 10.130297 seconds\n",
      "iteration 49 finished in 10.288371 seconds\n",
      "iteration 50 finished in 9.980942 seconds\n",
      "iteration 51 finished in 10.096384 seconds\n",
      "iteration 52 finished in 10.181648 seconds\n",
      "iteration 53 finished in 9.705166 seconds\n",
      "iteration 54 finished in 10.149527 seconds\n",
      "iteration 55 finished in 10.041820 seconds\n",
      "iteration 56 finished in 10.560634 seconds\n",
      "iteration 57 finished in 10.461403 seconds\n",
      "iteration 58 finished in 9.931587 seconds\n",
      "iteration 59 finished in 9.783609 seconds\n",
      "iteration 60 finished in 9.727843 seconds\n",
      "iteration 61 finished in 9.747305 seconds\n",
      "iteration 62 finished in 9.744332 seconds\n",
      "iteration 63 finished in 9.722861 seconds\n",
      "iteration 64 finished in 9.789928 seconds\n",
      "iteration 65 finished in 9.750950 seconds\n",
      "iteration 66 finished in 9.948959 seconds\n",
      "iteration 67 finished in 10.363140 seconds\n",
      "iteration 68 finished in 10.498031 seconds\n",
      "iteration 69 finished in 10.607986 seconds\n",
      "iteration 70 finished in 10.311024 seconds\n",
      "iteration 71 finished in 9.771327 seconds\n",
      "iteration 72 finished in 9.711904 seconds\n",
      "iteration 73 finished in 9.775953 seconds\n",
      "iteration 74 finished in 10.299153 seconds\n",
      "iteration 75 finished in 10.124906 seconds\n",
      "iteration 76 finished in 10.308541 seconds\n",
      "iteration 77 finished in 10.357036 seconds\n",
      "iteration 78 finished in 10.157392 seconds\n",
      "iteration 79 finished in 9.963170 seconds\n",
      "iteration 80 finished in 9.748555 seconds\n",
      "iteration 81 finished in 9.724031 seconds\n",
      "iteration 82 finished in 9.825106 seconds\n",
      "iteration 83 finished in 9.733472 seconds\n",
      "iteration 84 finished in 9.670559 seconds\n",
      "iteration 85 finished in 9.720442 seconds\n",
      "iteration 86 finished in 9.748956 seconds\n",
      "iteration 87 finished in 9.719485 seconds\n",
      "iteration 88 finished in 9.837183 seconds\n",
      "iteration 89 finished in 9.854201 seconds\n",
      "iteration 90 finished in 9.907169 seconds\n",
      "iteration 91 finished in 10.072752 seconds\n",
      "iteration 92 finished in 10.446685 seconds\n",
      "iteration 93 finished in 9.790944 seconds\n",
      "iteration 94 finished in 9.815636 seconds\n",
      "iteration 95 finished in 9.792572 seconds\n",
      "iteration 96 finished in 9.727198 seconds\n",
      "iteration 97 finished in 9.644349 seconds\n",
      "iteration 98 finished in 9.898575 seconds\n",
      "iteration 99 finished in 9.865463 seconds\n",
      "iteration 100 finished in 9.869500 seconds\n"
     ]
    }
   ],
   "source": [
    "user_vector_matrix, item_vector_matrix = train_model(125, 100, 0, user_property_booking_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaeee0e6f3a4010870a6145827d1e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_path = os.path.join(OUTPUT_PATH, \"final_tuned_pred_unordered_latent_factor.pickle\")\n",
    "results_final_tuned = compute_scores_for_df(train_processed, test_processed, user_parameters, user_vector_matrix, item_vector_matrix, results_path, k_users=5)\n",
    "    \n",
    "results_final_tuned_ordered = results_final_tuned.sort_values(by=['srch_id', 'score'], ascending = [True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>prop_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259647</th>\n",
       "      <td>-2147403968</td>\n",
       "      <td>287623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259631</th>\n",
       "      <td>-2147403968</td>\n",
       "      <td>278277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259625</th>\n",
       "      <td>-2147403968</td>\n",
       "      <td>270584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259632</th>\n",
       "      <td>-2147403968</td>\n",
       "      <td>364047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259644</th>\n",
       "      <td>-2147403968</td>\n",
       "      <td>289794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129511</th>\n",
       "      <td>2146902698</td>\n",
       "      <td>3885482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129526</th>\n",
       "      <td>2146902698</td>\n",
       "      <td>256516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129519</th>\n",
       "      <td>2146902698</td>\n",
       "      <td>268299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129505</th>\n",
       "      <td>2146902698</td>\n",
       "      <td>292812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129487</th>\n",
       "      <td>2146902698</td>\n",
       "      <td>257087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351147 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           srch_id  prop_key\n",
       "259647 -2147403968    287623\n",
       "259631 -2147403968    278277\n",
       "259625 -2147403968    270584\n",
       "259632 -2147403968    364047\n",
       "259644 -2147403968    289794\n",
       "...            ...       ...\n",
       "129511  2146902698   3885482\n",
       "129526  2146902698    256516\n",
       "129519  2146902698    268299\n",
       "129505  2146902698    292812\n",
       "129487  2146902698    257087\n",
       "\n",
       "[351147 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_final_tuned = ensemble_model(train_processed, results_final_tuned_ordered, 1.5, 0.1, -10, 0.2)\n",
    "ensemble_final_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_final_tuned.to_csv('output/ensemble_model_final_tuned_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Challanges and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most users have only one booking\n",
    "- Test set is mostly unseen users\n",
    "- Implicit feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use locality-sensitive hashing to speed up the process of finding similar users\n",
    "- Hypermeter tuning\n",
    "- Try incorporating LambdaMART model\n",
    "- Try incorporating user preference as features and use logistic and random forest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
